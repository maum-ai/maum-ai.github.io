---
layout: post
title: "[Paper Review] Activate or Not: Learning Customized Activation"
description: "Swishì™€ ReLUì™€ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ê³ , í•™ìŠµì´ ê°€ëŠ¥í•œ í™œì„±í•¨ìˆ˜ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤."
categories: paper-review
author: Hyoung-Kyu Song
github: deepkyu
use_math: true
---
# Activate or Not: Learning Customized Activation

> Ma, Ningning, et al. "Activate or Not: Learning Customized Activation."  
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.

## TL;DR

- í•œì¤„. Activation functionë“¤ì— ëŒ€í•´ ê¸°ì¡´ Maxout familyì— í•´ë‹¹í•˜ëŠ” ì¼ë°˜í™”ë¥¼ ë„˜ì–´ ACON Familyë¼ëŠ” ê°œë…ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì¼ë°˜í™”ë¥¼ í•©ë‹ˆë‹¤.
- ë‘ì¤„. ì´ë¥¼ í†µí•´ ACON Familyì—ì„œ ê° activationì„ ê²°ì • ì§“ëŠ” parameter ìì²´ë¥¼ learnableí•˜ê²Œ í•˜ì—¬ acon ì´ë¼ëŠ” activationì„ ìƒˆë¡­ê²Œ ì œì‹œí•©ë‹ˆë‹¤.
- ì„¸ì¤„. ê¸°ì¡´ Swish ëŠ” NASë¡œ ì°¾ì€ activationìœ¼ë¡œì„œ, ë” ì¢‹ë‹¤ëŠ” ê²ƒë§Œ ì•Œ ë¿, ì™œ ì¢‹ì€ì§€ë¥¼ ëª°ëëŠ”ë°, ACON Familyì— ëŒ€ì‘í•˜ì—¬ ë´¤ì„ ë•Œ, ì´ë¥¼ ì–´ëŠì •ë„ ì„¤ëª…í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

## ë¨¼ì € ì•Œë©´ ì¢‹ì€ ê²ƒë“¤

### Swish Activation Function

$$\operatorname{swish}(x):=x \times \operatorname{sigmoid}(\beta x)=\frac{x}{1+e^{-\beta x}}$$

![figure1_swish.png](/assets/2021-07-12-paper-review-acon/figure1_swish.png)

- Linear Functionê³¼ ReLU ì‚¬ì´ì—ì„œì˜ non-linearly interpolated activation
    - With Î²=0, the functions turns into the scaled linear function f(x)=x/2.
    - With Î²â†’âˆ, the sigmoid component approaches a 0-1 function, so swish becomes like the ReLU function.
    - For Î²=1, the function becomes equivalent to the Sigmoid-weighted Linear Unit (SiL) function used in reinforcement learning.
    - Î² is either constant or a trainable parameter depending on the model.
- ì°½í˜¸ë‹˜ì´ ì¢‹ì•„í•˜ì‹œëŠ” Activation Functionì´ê¸°ë„ í•˜ì£  ğŸ™‚
- Generative Modelì—ì„œë„ ReLU ëŒ€ì‹  ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ì´ ìˆìŠµë‹ˆë‹¤.
- ìµœê·¼ì—ëŠ” Implicit Representation Network ìƒì—ì„œë„ Swishê°€ ì£¼ëª©ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤. (SIRENê°™ì´ periodic function ê¹Œì§€ í•„ìš”ì—†ë‹¤)

### Sigmoid

$$S(x)=\frac{1}{1+e^{-x}}$$

![figure2_sigmoid.png](/assets/2021-07-12-paper-review-acon/figure2_sigmoid.png)


[https://www.kaggle.com/general/197117](https://www.kaggle.com/general/197117)

- ì—¬ê¸°ì„œëŠ” Activationìœ¼ë¡œ ì‹œì‚¬í•˜ê¸°ë³´ë‹¤ëŠ” ìˆ˜ì‹ í‘œí˜„ ì‹œì— sigmoidë¡œ ë¬¶ì–´ í‘œí˜„í•˜ê¸° ìœ„í•´ í™•ì¸í•˜ê³  ë„˜ì–´ê°€ì•¼ í•©ë‹ˆë‹¤.
- Swishê°€ ê²°êµ­ **input ê°’ì— sigmoidí•œ ê²ƒ(Î²ë¥¼ ê³±í•˜ê¸°ëŠ” í•˜ê² ì§€ë§Œ)ê³¼ input ê°’ì˜ ê³±ìœ¼ë¡œ í‘œí˜„ëœë‹¤**ëŠ” ê²ƒë„ ë‹¤ì‹œ í•œë²ˆ ë¦¬ë§ˆì¸ë“œí•˜ê³  ë„˜ì–´ê°‘ì‹œë‹¤ ğŸ˜

### Maxout Family

![figure3_maxout.png](/assets/2021-07-12-paper-review-acon/figure3_maxout.png)


- ReLUì™€ ê°™ì€ Activation Functionì— ì†ì„ ë†’ê²Œ ë“¤ì–´ì£¼ë˜ ë…¼ë¬¸ì…ë‹ˆë‹¤.
- Goodfellowì™€ Bengioì˜ ë…¼ë¬¸ìœ¼ë¡œ, Maximumì„ ì„ íƒí•˜ëŠ” ê²ƒìœ¼ë¡œë„ ì„ì˜ì˜ Convex Functionì— ëŒ€í•´ ë‘ë£¨ ê·¼ì‚¬í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## Main Idea

### ACON(**Ac**tivationOrNot) Activation Function

$$\operatorname{ACON-C}(x):=\left(p_{1}-p_{2}\right) x \cdot \sigma\left(\beta\left(p_{1}-p_{2}\right) x\right)+p_{2} x$$

![figure4_acon.png](/assets/2021-07-12-paper-review-acon/figure4_acon.png)


ì €ìëŠ” ACON(ë” ë‚˜ì•„ê°€ì„œ Meta-ACON)ì´ë¼ê³  í•˜ëŠ” Activation Functionì„ ì œì•ˆí•©ë‹ˆë‹¤. ACON activationì€ trainableí•œ activationìœ¼ë¡œ Neuronì„ Activationí•  ì§€ ì•ˆ í• ì§€ë¥¼ ê° Layerì˜ íŠ¹ì„±ì— ë§ê²Œ ê²°ì •í•©ë‹ˆë‹¤.

- ì™¼ìª½: ReLU Activationì„ ì‚¬ìš©í•˜ì˜€ì„ ë•Œë¥¼ ë³´ì—¬ì£¼ëŠ” ê·¸ë¦¼
- ì˜¤ë¥¸ìª½: ACON Activationì„ ì‚¬ìš©í•˜ì˜€ì„ ë•Œ, íŠ¹ì • Layerì˜ Activationì´ ì‘ë™ ì•ˆ í•  ìˆ˜ë„ (Linear í•˜ê²Œ pass), ì‘ë™ í•  ìˆ˜ë„ (Non-linear Activation) ìˆë‹¤ëŠ” ê±¸ ë³´ì—¬ì£¼ëŠ” ê·¸ë¦¼

### ì–´ë–»ê²Œ í•´ì„œ ACON ì‹ì„ ë„ì¶œí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆì„ê¹Œìš”?

ë¨¼ì € Maximum Function(max(x1, ..., xn))ì— ëŒ€í•´ smoothëœ ë²„ì „ì„ ë³´ì•„ì•¼ í•©ë‹ˆë‹¤. Maximumì„ êµ¬í•œë‹¤ëŠ” ê²ƒì€ ì¼ë°˜ì ìœ¼ë¡œ differentiableí•˜ì§€ ì•Šì§€ë§Œ, ì´ë¥¼ smoothí•œ í•¨ìˆ˜ëŠ” differentiableí•˜ê²Œ ë©ë‹ˆë‹¤.

ë³´í†µ ì•„ë˜ì˜ ì‹ì²˜ëŸ¼ í‘œí˜„í•©ë‹ˆë‹¤.

$$S_{\beta}\left(x_{1}, \ldots, x_{n}\right)=\frac{\sum_{i=1}^{n} x_{i} e^{\beta x_{i}}}{\sum_{i=1}^{n} e^{\beta x_{i}}}$$

ì´ ë•Œ, Î² ëŠ” switching factorë¡œì„œ

- With Î² â†’ âˆ, ì£¼ì–´ì§„ í•¨ìˆ˜ëŠ” Maximum Function ì˜ ì—­í• ì„ í•˜ê²Œ ë©ë‹ˆë‹¤.
- With Î² â†’ 0, ì£¼ì–´ì§„ í•¨ìˆ˜ëŠ” ì‚°ìˆ í‰ê· (Arithmetic Mean, ìš°ë¦¬ê°€ ì¼ë°˜ì ìœ¼ë¡œ ì•„ëŠ” í‰ê· )ì²˜ëŸ¼ ì‘ë™í•©ë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ Neural Networkì—ì„œ ë§ì´ ì‚¬ìš©í•˜ëŠ” Activation Functionë“¤ì€ ìœ„ì—ì„œ ì–¸ê¸‰í•œ Maxoutì— ì¤€í•˜ëŠ” ê²ƒìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$max(Î·a(x), Î·b(x))Â $$

ì˜ˆë¥¼ ë“¤ì–´, ReLUëŠ” Î·a(x)=x, Î·b(x)=0ì¸ ê²ƒìœ¼ë¡œ ìƒê°í•˜ë©´, ì´ ì—­ì‹œ Maxout Familyì— ì†í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Leaky ReLU, FReLU ë“±ë„ ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•´ë³´ë©´ ëª¨ë‘ Maxout Familyì— ì†í•˜ê²Œ ë©ë‹ˆë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œì˜ ëª©í‘œëŠ” Maximum Functionê³¼ ìœ„ Maxout Familyë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬, Maxout Family ê°ê°ì— ìƒì‘í•˜ëŠ” activation functionë“¤ì„ smoothí•œ í•¨ìˆ˜ë¡œ ê·¼ì‚¬í•´ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤. ìœ„ì—ì„œ Smoothëœ Maximum Functionì„ ì‘ì„±í•  ë•Œ, ì…ë ¥ ê°’ì˜ ê°œìˆ˜ë¥¼ 2ê°œë¡œë§Œ í•œì •í•´ì„œ ì‹ì„ ì „ê°œí•˜ë©´ ë”±ì´ê² ë„¤ìš”! ~~ë‘˜ ì¤‘ì— í•˜ë‚˜ë§Œ ê³¨ë¼ (Yes or Yes)~~

$$\begin{array}{l}S_{\beta}\left(\eta_{a}(x), \eta_{b}(x)\right) \\=\eta_{a}(x) \cdot \frac{e^{\beta \eta_{a}(x)}}{e^{\beta \eta_{a}(x)}+e^{\beta \eta_{b}(x)}}+\eta_{b}(x) \cdot \frac{e^{\beta \eta_{b}(x)}}{e^{\beta \eta_{a}(x)}+e^{\beta \eta_{b}(x)}} \\=\eta_{a}(x) \cdot \frac{1}{1+e^{-\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)}}+\eta_{b}(x) \cdot \frac{1}{1+e^{-\beta\left(\eta_{b}(x)-\eta_{a}(x)\right)}} \\=\eta_{a}(x) \cdot \sigma\left[\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)\right]+\eta_{b}(x) \cdot \sigma\left[\beta\left(\eta_{b}(x)-\eta_{a}(x)\right)\right] \\=\left(\eta_{a}(x)-\eta_{b}(x)\right) \cdot \sigma\left[\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)\right]+\eta_{b}(x)\end{array}$$

ì¦‰, Smoothëœ Maximum Functionì— ëŒ€ì…í•´ì„œ ì „ê°œí•´ë³´ë©´

$$\begin{array}{l}S_{\beta}\left(\eta_{a}(x), \eta_{b}(x)\right) =\left(\eta_{a}(x)-\eta_{b}(x)\right) \cdot \sigma\left[\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)\right]+\eta_{b}(x)\end{array}$$

ì´ ë  ê²ƒì…ë‹ˆë‹¤!

### ACON ì•ˆì— Swish ìˆë‹¤ ğŸ˜‰

ì, ê·¸ëŸ¼ ì•„ê¹Œ ì–¸ê¸‰í•œ Maxout Familyì— ì¤€í•˜ì—¬ ì—¬ëŸ¬ Activationì„ í‘œí˜„í•  ìˆ˜ ìˆì—ˆë‹¤ë©´, ê°ê°ì„ Smoothëœ Maximum Functionì— í•´ë‹¹í•˜ë„ë¡ ì „ê°œë¥¼ í•´ë³¼ê¹Œìš”?

![figure5_maxout_family_acon_family.png](/assets/2021-07-12-paper-review-acon/figure5_maxout_family_acon_family.png)


ReLUì˜ smoothë˜ëŠ” ë²„ì „ì´ Swishë¼ëŠ” ê±´ ì§ê´€ìœ¼ë¡œë„ ë§ì´ë“¤ ì´í•´í•˜ê³  ìˆì—ˆëŠ”ë°ìš”. ì´ ì‹ì—ì„œ ë³´ë“¯ì´ Smoothëœ Maximum Functionì— ëŒ€ì…í•´ì„œ ì „ê°œí•´ë³´ë©´, ë°”ë¡œ Swish ì‹ì´ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤. ì €ìëŠ” ì´ë¥¼ í†µí•´ Swishê°€ ReLUì˜ Smooth Approximationì„ì„ í‘œí˜„í•  ìˆ˜ ìˆê²Œ ëœë‹¤ê³  ë§í•©ë‹ˆë‹¤.

ë˜í•œ, Leaky ReLUì˜ ìƒìœ„ í˜¸í™˜ì´ê¸°ë„ í•œ PReLU(Parametric ReLU, ìŒìˆ˜ ë¶€ë¶„ì˜ ê¸°ìš¸ê¸° ê°’ì´ learnableí•¨)ë„ ì‚´í´ë³´ë©´, ì—­ì‹œ Smoothë˜ëŠ” í•¨ìˆ˜ë¡œ ëŒ€ì‘í•˜ëŠ” ê²ƒì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì•„ ì´ ë•Œ PReLUì— ëŒ€ì‘í•˜ë ¤ë©´, p < 1 ì¸ ê±¸ë¡œ í•œì •í•´ì„œ ìƒê°í•´ë´ìš” ìš°ë¦¬ ğŸ™‚)

ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ ê° ì„ í˜• í•¨ìˆ˜ì˜ ê°€ì¤‘ì¹˜(Cartesian ì¢Œí‘œê³„ ìƒì€ ê¸°ìš¸ê¸°ê² ì£ ?)ê°€ p1, p2ë¡œ í‘œí˜„í•˜ë©´ ê°€ì¥ ì¼ë°˜í™”ëœ í‘œí˜„ì¼í…ë°ìš” (p1 â‰  p2). ì—¬ê¸°ì— ê°ê° Maxout Family, ACON Familyë¥¼ ëŒ€ì‘í•´ë³´ë©´ ì¼ë°˜í™”ëœ ì‹ì´ ë‚˜ì˜µë‹ˆë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í•œ

$\operatorname{ACON-C}(x):=\left(p_{1}-p_{2}\right) x \cdot \sigma\left(\beta\left(p_{1}-p_{2}\right) x\right)+p_{2} x$

ì´ ì´ë ‡ê²Œ ìœ ë„ë˜ê²Œ ë˜ëŠ” ê²ƒì´ì£ !

ì‚¬ì‹¤ Maxout Familyì—ì„œ ë¹„êµí•˜ê²Œ ë˜ëŠ” ë‘ í•¨ìˆ˜ëŠ” ìœ„ì—ì„œì²˜ëŸ¼ ë‹¨ìˆœí•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê°ê°ì´ ë³µì¡í•´ì§ˆìˆ˜ë¡ ë” ë§ì€ í•¨ìˆ˜ë“¤ì„ í‘œí˜„í•  ìˆ˜ ìˆê²Œ ë˜ì£ . ë‹¤ë§Œ, ì €ìëŠ” ì´ Maxout Familyë¥¼ ACON Familyë¡œ ë°”ê¿¨ì„ ë•Œ(ì¦‰, Smooth Maximum Functionìœ¼ë¡œ ê·¼ì‚¬í–ˆì„ ë•Œ)ì˜ íš¨ê³¼ë¥¼ ë³´ëŠ” ë°ì— ì—°êµ¬ë¥¼ ì§‘ì¤‘í–ˆë‹¤ê³  í•´ìš”. í–¥í›„ ì—°êµ¬ì—ì„œ ë” ì „ì²´ì ì¸ Scopeì—ì„œì˜ ë¹„êµê°€ ìˆê¸°ë¥¼ ê¸°ëŒ€í•´ë´…ë‹ˆë‹¤!

### ACONì˜ íŠ¹ì„±

ACONì— íŠ¹ì • ê°’ì„ ëŒ€ì…í•´ì„œ í•œë²ˆ ì‚´í´ë³¼ê¹Œìš”?

![figure6_acon_example.png](/assets/2021-07-12-paper-review-acon/figure6_acon_example.png)


p1=1.2, p2=-0.8ì¼ ë•Œ ACON-Cì— ëŒ€ì‘í•˜ëŠ” ì‹ì„ ì—¬ëŸ¬ Î²ê°’ì— ëŒ€í•´ í‘œí˜„í•œ graphì…ë‹ˆë‹¤.

- Î²ê°€ í´ ë•ŒëŠ”, maximum functionì²˜ëŸ¼ ë°˜ì‘í•˜ì—¬ ë¹„ì„ í˜•ì ì¸ íŠ¹ì„±ì„ ê°–ê²Œ ë˜ê³ ìš”.
- Î²ê°€ 0ì— ê°€ê¹Œìš¸ ë•ŒëŠ” mean functionì— ê·¼ì‚¬ë˜ì–´ ì„ í˜•ì ì¸ íŠ¹ì„±ì„ ê°–ë„¤ìš”.

![figure7_acon_property.png](/assets/2021-07-12-paper-review-acon/figure7_acon_property.png)


ACON Activationê³¼ ì´ì— ëŒ€í•œ ë„í•¨ìˆ˜(derivative)ë¥¼ ë³´ì—¬ì£¼ëŠ” ê·¸ë¦¼ì…ë‹ˆë‹¤.

- ì™¼ìª½: Î²ê°€ fixed ë˜ì–´ ìˆì„ ë•Œ, p1, p2 ê³„ìˆ˜ì— ë”°ë¼ ì–´ë–»ê²Œ Activation functionì´ ë‹¬ë¼ì§€ëŠ” ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
- ê°€ìš´ë°: Î² ê°’ì´ ë‹¬ë¼ì§ì— ë”°ë¼ ACONì˜ ë„í•¨ìˆ˜ê°€ ë³€í™”í•˜ê²Œ ë˜ê³  ì´ë¥¼ í†µí•´ Î²ì˜ ì—­í• ì„ ì§ì‘í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. TODO
- ì˜¤ë¥¸ìª½: Î²ê°€ fixed ë˜ì–´ ìˆì„ ë•Œ, p1, p2 ê³„ìˆ˜ì— ë”°ë¼ ACONì˜ ë„í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ” ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. TODO

ACONì˜ ë„í•¨ìˆ˜ë¥¼ ë³´ë©´ì„œ ì•„ë˜ì™€ ê°™ì€ ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆì–´ìš”.

- p1, p2ëŠ” ê°ê° Upper/Lower Boundì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ê²°ì •í•˜ê²Œ ë©ë‹ˆë‹¤.
- Î² ê°’ì€ ë„í•¨ìˆ˜ ìƒì—ì„œ p1, p2ì— ì˜í•´ ê²°ì •ëœ Upper/Lower Boundì— ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ê·¼ì‚¬ë˜ëŠ” ì§€ë¥¼ ê²°ì •í•˜ê²Œ ë©ë‹ˆë‹¤.

Swishì—ì„œëŠ” Hyperparameter Î²ë§Œì´ Upper/Lower Boundì— ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ê·¼ì‚¬ë˜ëŠ” ì§€ë¥¼ ê²°ì •í•˜ê²Œ ë˜ëŠ”ë°ìš”. ACONì—ì„œëŠ” p1, p2ê°€ ì´ Bound ê°’ì„ ê²°ì •í•˜ê²Œ ë˜ê³ , ì´ ì—­ì‹œ learnableí•´ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” íŠ¹ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ boundaryê°€ learnableí•˜ë‹¤ëŠ” ê²ƒì€ optimizationì„ ì‰½ê²Œ í•˜ëŠ” ë°ì— í•„ìˆ˜ì ì¸ íŠ¹ì„±ì´ê³ , ì €ìëŠ” ì´ ì¥ì ì„ ì‹¤í—˜ ê²°ê³¼ë¥¼ í†µí•´ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.

### í•™ìŠµì— ëª¨ë‘ ë§¡ê²¨ë²„ë¦¬ì! Meta-ACON

Meta-ACONì€ Î² ìì²´ë¥¼ Learnableí•œ parameterë¡œ ë†”ë‘ëŠ” ê²ƒì—ì„œ ë” ë‚˜ì•„ê°€, Layerì— ì…ë ¥ë˜ëŠ” feature mapìœ¼ë¡œë¶€í„° FC Layersë¥¼ ê±°ì³ estimation ë˜ë„ë¡ ë§Œë“  ê²ƒì…ë‹ˆë‹¤.

![figure8_meta_acon_distribution.png](/assets/2021-07-12-paper-review-acon/figure8_meta_acon_distribution.png)


ACONê³¼ meta-ACONì„ ë¹„êµí•œ ë„ì‹ì…ë‹ˆë‹¤. ResNet50ì˜ ë§ˆì§€ë§‰ BottleNeck Layerì—ì„œì˜ activationì„ ë¹„êµí•œ ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ 7ê°œì˜ sampleì„ ì„ì˜ë¡œ ì¶”ì¶œí•´ë´¤ìŠµë‹ˆë‹¤.

- ACONì—ì„œ ì¶”ì¶œí•  ê²½ìš°, íŒŒë€ íˆìŠ¤í† ê·¸ë¨ì— í•´ë‹¹í•˜ëŠ”ë°ìš”. 7ê°œì˜ sampleì´ ë™ì¼í•œ Î² distributionì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
- Meta-ACONì—ì„œëŠ” 7ê°œì˜ sampleì´ ì„œë¡œ ë‹¤ë¥¸ Î² distributionì„ ë³´ì—¬ì£¼ê²Œ ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ Î² ê°’ì´ ì‘ì„ìˆ˜ë¡, ì„ í˜•ì ìœ¼ë¡œ(linear) ë°˜ì‘í•˜ëŠ” ê²ƒì´ê³ , Î² ê°’ì´ í´ ìˆ˜ë¡ ë¹„ì„ í˜•ì (non-linear)ìœ¼ë¡œ ë°˜ì‘í•˜ê³  ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.

Code Snippetìœ¼ë¡œ ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

[https://gist.github.com/deepkyu/1616637a06e1b00534a7557c35ad2209](https://gist.github.com/deepkyu/1616637a06e1b00534a7557c35ad2209)

[https://gist.github.com/deepkyu/77b2e5acd98969fdb21ea22198954ad5](https://gist.github.com/deepkyu/77b2e5acd98969fdb21ea22198954ad5)

### ê²°ê³¼

![figure9_result1.png](/assets/2021-07-12-paper-review-acon/figure9_result1.png)

ImageNet Classificationì— ëŒ€í•œ ShuffleNetV2 ê¸°ì¤€ ê²°ê³¼

![figure10_result2.png](/assets/2021-07-12-paper-review-acon/figure10_result2.png)

ì—¬ëŸ¬ backboneì— ëŒ€í•´ ImageNet accuracyë¥¼ ë†’ì¼ ìˆ˜ ìˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ ì»¤ì§ˆ ìˆ˜ë¡, Meta-ACONì„ ì‚¬ìš©í•  ìˆ˜ë¡ Accuracy í–¥ìƒì´ í½ë‹ˆë‹¤. (Swish ëŒ€ì²´, SENet Novelty ì¶”ê°€ ë“± ëŒ€ë¹„)

![figure11_result3.png](/assets/2021-07-12-paper-review-acon/figure11_result3.png)

![figure12_result4.png](/assets/2021-07-12-paper-review-acon/figure12_result4.png)

![figure13_result5.png](/assets/2021-07-12-paper-review-acon/figure13_result5.png)


## í•¨ê»˜ ì½ìœ¼ë©´ ì¢‹ì€ ë…¼ë¬¸

[1] [Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. "Searching for activation functions." arXiv preprint arXiv:1710.05941 (2017).](https://arxiv.org/abs/1710.05941)

[2] [Goodfellow, Ian, et al. "Maxout networks." International conference on machine learning. PMLR, 2013.](http://proceedings.mlr.press/v28/goodfellow13.html)