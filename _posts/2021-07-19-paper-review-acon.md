---
layout: post
title: "Activate or Not: Learning Customized Activation"
description: "Swishì™€ ReLUì™€ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ê³ , í•™ìŠµì´ ê°€ëŠ¥í•œ í™œì„±í•¨ìˆ˜ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤"
categories: paper-review
author: Hyoung-Kyu Song
github: deepkyu
use_math: true
---

[![arXiv](https://img.shields.io/badge/arXiv-2009.04759-brightgreen.svg?style=flat-square)](https://arxiv.org/abs/2009.04759)
[![githubio](https://img.shields.io/static/v1?message=Official%20Repo&logo=Github&labelColor=grey&color=blue&logoColor=white&label=%20&style=flat-square)](https://github.com/nmaac/acon)

> Ma, Ningning, et al. "Activate or Not: Learning Customized Activation."  
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.

### TL;DR

- Activation functionë“¤ì— ëŒ€í•´ ê¸°ì¡´ Maxout familyì— í•´ë‹¹í•˜ëŠ” ì¼ë°˜í™”ë¥¼ ë„˜ì–´ **ACON Family**ë¼ëŠ” ê°œë…ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì¼ë°˜í™”ë¥¼ í•©ë‹ˆë‹¤.
- ì´ë¥¼ í†µí•´ ACON Familyì—ì„œ ê° activationì„ ê²°ì • ì§“ëŠ” parameter ìì²´ë¥¼ learnableí•˜ê²Œ í•˜ì—¬ **acon** ì´ë¼ëŠ” activationì„ ìƒˆë¡­ê²Œ ì œì‹œí•©ë‹ˆë‹¤.
- ê¸°ì¡´ Swish ëŠ” NASë¡œ ì°¾ì€ activationìœ¼ë¡œì„œ, ë” ì¢‹ë‹¤ëŠ” ê²ƒë§Œ ì•Œ ë¿, ì™œ ì¢‹ì€ì§€ë¥¼ ëª°ëëŠ”ë°, **ACON Family**ì— ëŒ€ì‘í•˜ì—¬ ë´¤ì„ ë•Œ, ì´ë¥¼ ì–´ëŠì •ë„ ì„¤ëª…í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

## ë¨¼ì € ì•Œë©´ ì¢‹ì€ ê²ƒë“¤

### Swish Activation Function [<sup>[1]</sup>](#r1)

$$\operatorname{swish}(x):=x \times \sigma(\beta x)=\frac{x}{1+e^{-\beta x}}$$

![figure1_swish.png](/assets/2021-07-12-paper-review-acon/figure1_swish.png){: .center-image }

- Linear Functionê³¼ ReLU ì‚¬ì´ì—ì„œì˜ non-linearly interpolated activationì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
    - $Î² = 0$ ì¼ ê²½ìš°, Linear function $f(x) = x/2$ ì²˜ëŸ¼ ì‘ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.
    - ë°˜ëŒ€ë¡œ $Î² â†’ âˆ$ì¼ ê²½ìš°, Sigmoidì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì´ 0-1 activationì²˜ëŸ¼ ì‘ìš©í•˜ê²Œ ë˜ì–´, Swishê°€ ReLUì²˜ëŸ¼ ì‘ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.
    - $Î² = 1$ì¼ ê²½ìš°, ê°•í™”í•™ìŠµì—ì„œ ì‚¬ìš©ë˜ëŠ” Sigmoid-weighted Linear Unit (SiL) functionì²˜ëŸ¼ ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - $Î²$ëŠ” ìœ„ì—ì„œ ë³´ì‹  ê²ƒì²˜ëŸ¼ ì–´ë–¤ ìƒìˆ˜ì¼ ìˆ˜ë„ ìˆê³ , ëª¨ë¸ì— ë”°ë¼ì„œëŠ” í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
- ë¸Œë ˆì¸íŒ€ AI Scientistë¶„ë“¤ì´ ìì£¼ ì‚¬ìš©í•˜ì‹œëŠ” Activation Functionì´ê¸°ë„ í•˜ì£  ğŸ™‚
- Generative Modelì—ì„œë„ ReLU ëŒ€ì‹  ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ì´ ìˆìŠµë‹ˆë‹¤.
- ìµœê·¼ì—ëŠ” Implicit Representation Network ìƒì—ì„œë„ Swishê°€ ë‹¤ì‹œê¸ˆ ì£¼ëª©ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤.
    - SIRENì—ì„œ ì–¸ê¸‰í•˜ëŠ” periodic function activation (Sine í•¨ìˆ˜ ë“±) ë³´ë‹¤ Swishê°€ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” Taskê°€ ìˆìŠµë‹ˆë‹¤.

### Sigmoid

$$\sigma(x)=\frac{1}{1+e^{-x}}$$

- ì—¬ê¸°ì„œëŠ” Activationìœ¼ë¡œ ì‹œì‚¬í•˜ê¸°ë³´ë‹¤ëŠ” ìˆ˜ì‹ í‘œí˜„ ì‹œì— sigmoidë¡œ ë¬¶ì–´ í‘œí˜„í•˜ê¸° ìœ„í•´ í™•ì¸í•˜ê³  ë„˜ì–´ê°€ì•¼ í•©ë‹ˆë‹¤.
- Swishê°€ ê²°êµ­ **input ê°’ì— sigmoidí•œ ê²ƒê³¼ input ê°’ì˜ ê³±ìœ¼ë¡œ í‘œí˜„ëœë‹¤**(Î² ë¥¼ ê³±í•˜ê¸°ëŠ” í•˜ê² ì§€ë§Œ)ëŠ” ê²ƒë„ ë‹¤ì‹œ í•œë²ˆ ë¦¬ë§ˆì¸ë“œí•˜ê³  ë„˜ì–´ê°‘ì‹œë‹¤ ğŸ˜

### Maxout Family

- ReLUì™€ ê°™ì€ Activation Functionì˜ ì¶œë°œì ì— í•´ë‹¹í•˜ëŠ” ê°œë… ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
- Goodfellowì™€ Bengioì˜ ë…¼ë¬¸[<sup>[2]</sup>](#r2) ìœ¼ë¡œ, Maximumì„ ì„ íƒí•˜ëŠ” ê²ƒìœ¼ë¡œë„ ì„ì˜ì˜ Convex Functionì— ëŒ€í•´ ë‘ë£¨ ê·¼ì‚¬í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## Main Idea

### ACON(**Ac**tivationOrNot) Activation Function

$$\operatorname{ACON-C}(x) := \left(p_{1}-p_{2}\right) x \cdot \sigma\left(\beta\left(p_{1}-p_{2}\right) x\right)+p_{2} x$$

![figure4_acon.png](/assets/2021-07-12-paper-review-acon/figure4_acon.png){: .center-image .resize-image-large}
*ACON Activationì„ ì‚¬ìš©í•˜ì˜€ì„ ë•Œ, íŠ¹ì • Layerì˜ Activationì´ Linear í•˜ê²Œ passìˆ˜ë„, Non-linear Activationìœ¼ë¡œ í™œì„±ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤*

ì €ìëŠ” ACON(ë” ë‚˜ì•„ê°€ì„œ Meta-ACON)ì´ë¼ê³  í•˜ëŠ” Activation Functionì„ ì œì•ˆí•©ë‹ˆë‹¤. ACON activationì€ trainableí•œ activationìœ¼ë¡œ Neuronì„ Activationí•  ì§€ ì•ˆ í• ì§€ë¥¼ ê° Layerì˜ íŠ¹ì„±ì— ë§ê²Œ ê²°ì •í•©ë‹ˆë‹¤.



### ì–´ë–»ê²Œ í•´ì„œ ACON ì‹ì„ ë„ì¶œí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆì„ê¹Œìš”?

ë¨¼ì € Maximum Function $max(x1, ..., xn)$ ì— ëŒ€í•´ smoothëœ ë²„ì „ì„ ë³´ì•„ì•¼ í•©ë‹ˆë‹¤. Maximumì„ êµ¬í•œë‹¤ëŠ” ê²ƒì€ ì¼ë°˜ì ìœ¼ë¡œ differentiableí•˜ì§€ ì•Šì§€ë§Œ, ì´ë¥¼ smoothí•œ í•¨ìˆ˜ëŠ” differentiableí•˜ê²Œ ë©ë‹ˆë‹¤.

ë³´í†µ ì•„ë˜ì˜ ì‹ì²˜ëŸ¼ í‘œí˜„í•©ë‹ˆë‹¤.

$$S_{\beta}\left(x_{1}, \ldots, x_{n}\right)=\frac{\sum_{i=1}^{n} x_{i} e^{\beta x_{i}}}{\sum_{i=1}^{n} e^{\beta x_{i}}}$$

ì´ ë•Œ, $Î²$ ëŠ” switching factorë¡œì„œ

- $Î² â†’ âˆ$ì¼ ë•Œ, ì£¼ì–´ì§„ í•¨ìˆ˜ëŠ” Maximum Function ì˜ ì—­í• ì„ í•˜ê²Œ ë©ë‹ˆë‹¤.
- $Î² â†’ 0$ì¼ ë•Œ, ì£¼ì–´ì§„ í•¨ìˆ˜ëŠ” ì‚°ìˆ í‰ê· (Arithmetic Mean, ìš°ë¦¬ê°€ ì¼ë°˜ì ìœ¼ë¡œ ì•„ëŠ” í‰ê· )ì²˜ëŸ¼ ì‘ë™í•©ë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ Neural Networkì—ì„œ ë§ì´ ì‚¬ìš©í•˜ëŠ” Activation Functionë“¤ì€ ìœ„ì—ì„œ ì–¸ê¸‰í•œ Maxoutì— ì¤€í•˜ëŠ” ê²ƒìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$max( Î·a(x), Î·b(x))Â $$

ì˜ˆë¥¼ ë“¤ì–´, ReLUëŠ” $Î·a(x)=x$, $Î·b(x)=0$ì¸ ê²ƒìœ¼ë¡œ ìƒê°í•˜ë©´, ì´ ì—­ì‹œ Maxout Familyì— ì†í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
Leaky ReLU, FReLU ë“±ë„ ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•´ë³´ë©´ ëª¨ë‘ Maxout Familyì— ì†í•˜ê²Œ ë©ë‹ˆë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œì˜ ëª©í‘œëŠ” Maximum Functionê³¼ ìœ„ Maxout Familyë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬, Maxout Family ê°ê°ì— ìƒì‘í•˜ëŠ” activation functionë“¤ì„ smoothí•œ í•¨ìˆ˜ë¡œ ê·¼ì‚¬í•´ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤. ìœ„ì—ì„œ Smoothëœ Maximum Functionì„ ì‘ì„±í•  ë•Œ, ì…ë ¥ ê°’ì˜ ê°œìˆ˜ë¥¼ 2ê°œë¡œë§Œ í•œì •í•´ì„œ ì‹ì„ ì „ê°œí•˜ë©´ ë”±ì´ê² ë„¤ìš”!

$$\begin{array}{l}S_{\beta}\left(\eta_{a}(x), \eta_{b}(x)\right) \\=\eta_{a}(x) \cdot \frac{e^{\beta \eta_{a}(x)}}{e^{\beta \eta_{a}(x)}+e^{\beta \eta_{b}(x)}}+\eta_{b}(x) \cdot \frac{e^{\beta \eta_{b}(x)}}{e^{\beta \eta_{a}(x)}+e^{\beta \eta_{b}(x)}} \\=\eta_{a}(x) \cdot \frac{1}{1+e^{-\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)}}+\eta_{b}(x) \cdot \frac{1}{1+e^{-\beta\left(\eta_{b}(x)-\eta_{a}(x)\right)}} \\=\eta_{a}(x) \cdot \sigma\left[\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)\right]+\eta_{b}(x) \cdot \sigma\left[\beta\left(\eta_{b}(x)-\eta_{a}(x)\right)\right] \\=\left(\eta_{a}(x)-\eta_{b}(x)\right) \cdot \sigma\left[\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)\right]+\eta_{b}(x)\end{array}$$

ì¦‰, Smoothëœ Maximum Functionì— ëŒ€ì…í•´ì„œ ì „ê°œí•´ë³´ë©´

$$\begin{array}{l}S_{\beta}\left(\eta_{a}(x), \eta_{b}(x)\right) =\left(\eta_{a}(x)-\eta_{b}(x)\right) \cdot \sigma\left[\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)\right]+\eta_{b}(x)\end{array}$$

ì´ ë  ê²ƒì…ë‹ˆë‹¤!

### ACON ì•ˆì— Swish ìˆë‹¤ ğŸ˜‰

ì, ê·¸ëŸ¼ ì•„ê¹Œ ì–¸ê¸‰í•œ Maxout Familyì— ì¤€í•˜ì—¬ ì—¬ëŸ¬ Activationì„ í‘œí˜„í•  ìˆ˜ ìˆì—ˆë‹¤ë©´, ê°ê°ì„ Smoothëœ Maximum Functionì— í•´ë‹¹í•˜ë„ë¡ ì „ê°œë¥¼ í•´ë³¼ê¹Œìš”?

![figure5_maxout_family_acon_family.png](/assets/2021-07-12-paper-review-acon/figure5_maxout_family_acon_family.png){: .center-image .resize-image-large}


ReLUì˜ smoothë˜ëŠ” ë²„ì „ì´ Swishë¼ëŠ” ê±´ ì§ê´€ìœ¼ë¡œë„ ë§ì´ë“¤ ì´í•´í•˜ê³  ìˆì—ˆëŠ”ë°ìš”. ì´ ì‹ì—ì„œ ë³´ë“¯ì´ Smoothëœ Maximum Functionì— ëŒ€ì…í•´ì„œ ì „ê°œí•´ë³´ë©´, ë°”ë¡œ Swish ì‹ì´ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤. ì €ìëŠ” ì´ë¥¼ í†µí•´ Swishê°€ ReLUì˜ Smooth Approximationì„ì„ í‘œí˜„í•  ìˆ˜ ìˆê²Œ ëœë‹¤ê³  ë§í•©ë‹ˆë‹¤.

ë˜í•œ, Leaky ReLUì˜ ìƒìœ„ í˜¸í™˜ì´ê¸°ë„ í•œ PReLU(Parametric ReLU, ìŒìˆ˜ ë¶€ë¶„ì˜ ê¸°ìš¸ê¸° ê°’ì´ learnableí•¨)ë„ ì‚´í´ë³´ë©´, ì—­ì‹œ Smoothë˜ëŠ” í•¨ìˆ˜ë¡œ ëŒ€ì‘í•˜ëŠ” ê²ƒì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì•„ ì´ ë•Œ PReLUì— ëŒ€ì‘í•˜ë ¤ë©´, p < 1 ì¸ ê±¸ë¡œ í•œì •í•´ì„œ ìƒê°í•´ë´ìš” ìš°ë¦¬ ğŸ™‚)

ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ ê° ì„ í˜• í•¨ìˆ˜ì˜ ê°€ì¤‘ì¹˜(Cartesian ì¢Œí‘œê³„ ìƒì€ ê¸°ìš¸ê¸°ê² ì£ ?)ê°€ p1, p2ë¡œ í‘œí˜„í•˜ë©´ ê°€ì¥ ì¼ë°˜í™”ëœ í‘œí˜„ì¼í…ë°ìš” (p1 â‰  p2). ì—¬ê¸°ì— ê°ê° Maxout Family, ACON Familyë¥¼ ëŒ€ì‘í•´ë³´ë©´ ì¼ë°˜í™”ëœ ì‹ì´ ë‚˜ì˜µë‹ˆë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í•œ

$$\operatorname{ACON-C}(x):=\left(p_{1}-p_{2}\right) x \cdot \sigma\left(\beta\left(p_{1}-p_{2}\right) x\right)+p_{2} x$$

ì´ ì´ë ‡ê²Œ ìœ ë„ë˜ê²Œ ë˜ëŠ” ê²ƒì´ì£ !

ì‚¬ì‹¤ Maxout Familyì—ì„œ ë¹„êµí•˜ê²Œ ë˜ëŠ” ë‘ í•¨ìˆ˜ëŠ” ìœ„ì—ì„œì²˜ëŸ¼ ë‹¨ìˆœí•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê°ê°ì´ ë³µì¡í•´ì§ˆìˆ˜ë¡ ë” ë§ì€ í•¨ìˆ˜ë“¤ì„ í‘œí˜„í•  ìˆ˜ ìˆê²Œ ë˜ì£ . ë‹¤ë§Œ, ì €ìëŠ” ì´ Maxout Familyë¥¼ ACON Familyë¡œ ë°”ê¿¨ì„ ë•Œ(ì¦‰, Smooth Maximum Functionìœ¼ë¡œ ê·¼ì‚¬í–ˆì„ ë•Œ)ì˜ íš¨ê³¼ë¥¼ ë³´ëŠ” ë°ì— ì—°êµ¬ë¥¼ ì§‘ì¤‘í–ˆë‹¤ê³  í•´ìš”. í–¥í›„ ì—°êµ¬ì—ì„œ ë” ì „ì²´ì ì¸ Scopeì—ì„œì˜ ë¹„êµê°€ ìˆê¸°ë¥¼ ê¸°ëŒ€í•´ë´…ë‹ˆë‹¤!

### ACONì˜ íŠ¹ì„±

ACONì— íŠ¹ì • ê°’ì„ ëŒ€ì…í•´ì„œ í•œë²ˆ ì‚´í´ë³¼ê¹Œìš”?

![figure6_acon_example.png](/assets/2021-07-12-paper-review-acon/figure6_acon_example.png){: .center-image .resize-image-medium}


p1=1.2, p2=-0.8ì¼ ë•Œ ACON-Cì— ëŒ€ì‘í•˜ëŠ” ì‹ì„ ì—¬ëŸ¬ Î²ê°’ì— ëŒ€í•´ í‘œí˜„í•œ graphì…ë‹ˆë‹¤.

- Î²ê°€ í´ ë•ŒëŠ”, maximum functionì²˜ëŸ¼ ë°˜ì‘í•˜ì—¬ ë¹„ì„ í˜•ì ì¸ íŠ¹ì„±ì„ ê°–ê²Œ ë˜ê³ ìš”.
- Î²ê°€ 0ì— ê°€ê¹Œìš¸ ë•ŒëŠ” mean functionì— ê·¼ì‚¬ë˜ì–´ ì„ í˜•ì ì¸ íŠ¹ì„±ì„ ê°–ë„¤ìš”.

![figure7_acon_property.png](/assets/2021-07-12-paper-review-acon/figure7_acon_property.png){: .center-image }


ACON Activationê³¼ ì´ì— ëŒ€í•œ ë„í•¨ìˆ˜(derivative)ë¥¼ ë³´ì—¬ì£¼ëŠ” ê·¸ë¦¼ì…ë‹ˆë‹¤.

- ì™¼ìª½: Î²ê°€ fixed ë˜ì–´ ìˆì„ ë•Œ, p1, p2 ê³„ìˆ˜ì— ë”°ë¼ ì–´ë–»ê²Œ Activation functionì´ ë‹¬ë¼ì§€ëŠ” ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
- ê°€ìš´ë°: Î² ê°’ì´ ë‹¬ë¼ì§ì— ë”°ë¼ ACONì˜ ë„í•¨ìˆ˜ê°€ ë³€í™”í•˜ê²Œ ë˜ê³  ì´ë¥¼ í†µí•´ Î²ì˜ ì—­í• ì„ ì§ì‘í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. TODO
- ì˜¤ë¥¸ìª½: Î²ê°€ fixed ë˜ì–´ ìˆì„ ë•Œ, p1, p2 ê³„ìˆ˜ì— ë”°ë¼ ACONì˜ ë„í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ” ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. TODO

ACONì˜ ë„í•¨ìˆ˜ë¥¼ ë³´ë©´ì„œ ì•„ë˜ì™€ ê°™ì€ ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆì–´ìš”.

- p1, p2ëŠ” ê°ê° Upper/Lower Boundì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ê²°ì •í•˜ê²Œ ë©ë‹ˆë‹¤.
- Î² ê°’ì€ ë„í•¨ìˆ˜ ìƒì—ì„œ p1, p2ì— ì˜í•´ ê²°ì •ëœ Upper/Lower Boundì— ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ê·¼ì‚¬ë˜ëŠ” ì§€ë¥¼ ê²°ì •í•˜ê²Œ ë©ë‹ˆë‹¤.

Swishì—ì„œëŠ” Hyperparameter Î²ë§Œì´ Upper/Lower Boundì— ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ê·¼ì‚¬ë˜ëŠ” ì§€ë¥¼ ê²°ì •í•˜ê²Œ ë˜ëŠ”ë°ìš”. ACONì—ì„œëŠ” p1, p2ê°€ ì´ Bound ê°’ì„ ê²°ì •í•˜ê²Œ ë˜ê³ , ì´ ì—­ì‹œ learnableí•´ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” íŠ¹ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ boundaryê°€ learnableí•˜ë‹¤ëŠ” ê²ƒì€ optimizationì„ ì‰½ê²Œ í•˜ëŠ” ë°ì— í•„ìˆ˜ì ì¸ íŠ¹ì„±ì´ê³ , ì €ìëŠ” ì´ ì¥ì ì„ ì‹¤í—˜ ê²°ê³¼ë¥¼ í†µí•´ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.

### í•™ìŠµì— ëª¨ë‘ ë§¡ê²¨ë²„ë¦¬ì! Meta-ACON

Meta-ACONì€ Î² ìì²´ë¥¼ Learnableí•œ parameterë¡œ ë†”ë‘ëŠ” ê²ƒì—ì„œ ë” ë‚˜ì•„ê°€, Layerì— ì…ë ¥ë˜ëŠ” feature mapìœ¼ë¡œë¶€í„° FC Layersë¥¼ ê±°ì³ estimation ë˜ë„ë¡ ë§Œë“  ê²ƒì…ë‹ˆë‹¤.

![figure8_meta_acon_distribution.png](/assets/2021-07-12-paper-review-acon/figure8_meta_acon_distribution.png){: .center-image .resize-image-medium}


ACONê³¼ meta-ACONì„ ë¹„êµí•œ ë„ì‹ì…ë‹ˆë‹¤. ResNet50ì˜ ë§ˆì§€ë§‰ BottleNeck Layerì—ì„œì˜ activationì„ ë¹„êµí•œ ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ 7ê°œì˜ sampleì„ ì„ì˜ë¡œ ì¶”ì¶œí•´ë´¤ìŠµë‹ˆë‹¤.

- ACONì—ì„œ ì¶”ì¶œí•  ê²½ìš°, íŒŒë€ íˆìŠ¤í† ê·¸ë¨ì— í•´ë‹¹í•˜ëŠ”ë°ìš”. 7ê°œì˜ sampleì´ ë™ì¼í•œ Î² distributionì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
- Meta-ACONì—ì„œëŠ” 7ê°œì˜ sampleì´ ì„œë¡œ ë‹¤ë¥¸ Î² distributionì„ ë³´ì—¬ì£¼ê²Œ ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ Î² ê°’ì´ ì‘ì„ìˆ˜ë¡, ì„ í˜•ì ìœ¼ë¡œ(linear) ë°˜ì‘í•˜ëŠ” ê²ƒì´ê³ , Î² ê°’ì´ í´ ìˆ˜ë¡ ë¹„ì„ í˜•ì (non-linear)ìœ¼ë¡œ ë°˜ì‘í•˜ê³  ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.

Code Snippetìœ¼ë¡œ ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ë³¸ Snippetì€ ì €ìì˜ [official github](https://github.com/nmaac/acon)ì—ì„œ ë°œì·Œí–ˆìœ¼ë©°, í•´ë‹¹ Repositoryì—ì„œ ìì„¸í•œ ì½”ë“œë¥¼ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<script src="https://gist.github.com/deepkyu/1616637a06e1b00534a7557c35ad2209.js"></script>
<script src="https://gist.github.com/deepkyu/77b2e5acd98969fdb21ea22198954ad5.js"></script>

### ê²°ê³¼

| ImageNet Classification Result       | Accuracy Improvements       |
| --------------------------- | --------------------------- |
| ![figure9_result1.png](/assets/2021-07-12-paper-review-acon/figure9_result1.png){: .center-image } | ![figure10_result2.png](/assets/2021-07-12-paper-review-acon/figure10_result2.png){: .center-image } |

ImageNet Classificationì— ëŒ€í•œ ShuffleNetV2 ê¸°ì¤€ ê²°ê³¼ë¥¼ ì‚´í´ë³´ë©´, í•™ìŠµ ì†ë„ë„ ë¹ ë¥¼ ë¿ë”ëŸ¬, Meta-ACONì„ ì‚¬ìš©í–ˆì„ ë•Œ Error rateê°€ ë‚®ì•„ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì „ë°˜ì ìœ¼ë¡œ ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ ì»¤ì§ˆ ìˆ˜ë¡, Meta-ACONì„ ì‚¬ìš©í•  ìˆ˜ë¡ Accuracy í–¥ìƒì´ í½ë‹ˆë‹¤. (Swish ëŒ€ì²´, SENet Novelty ì¶”ê°€ ë“± ëŒ€ë¹„)

<!-- ![figure11_result3.png](/assets/2021-07-12-paper-review-acon/figure11_result3.png){: .center-image } -->

| ![figure12_result4.png](/assets/2021-07-12-paper-review-acon/figure12_result4.png){: .center-image } | ![figure13_result5.png](/assets/2021-07-12-paper-review-acon/figure13_result5.png){: .center-image } |

ì´ë ‡ê²Œ Meta-ACONì€ ë‹¤ë¥¸ activation ëŒ€ë¹„ ImageNet Classificationì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì œí•œì ì´ê¸°ëŠ” í•˜ë‚˜, íŠ¹ì • backboneì— ëŒ€í•´ì„œ Object Detection ë° Semantic Segmentationì— ìˆì–´ì„œë„ ë‹¤ë¥¸ activation functionì„ ì‚¬ìš©í•  ë•Œë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### ë§ˆë¬´ë¦¬

ì´ë ‡ê²Œ ì˜¤ëŠ˜ì€ ReLUì™€ Swish ê°„ì˜ ê´€ê³„ë¥¼ í†µí•´ ìƒˆë¡œìš´ Activation Functionë“¤ì´ í¬ì§„ë˜ì–´ ìˆì„ë§Œí•œ ì¼ë°˜í™”ëœ ì‹ì„ ì°¾ê³ (ACON Family), ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Trainableí•œ Activation Functionì„ ìƒˆë¡œ ë§Œë‚˜ë³¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.  
ì‚¬ì‹¤ ì´ë ‡ê²Œ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ Activation Functionì´ ACONë§Œ ì²˜ìŒì¸ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ë˜í•œ, ì—¬ëŸ¬ Sub-taskì— ëŒ€í•´ ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” Activation Functionì¼ì§€ëŠ” ë¯¸ì§€ìˆ˜ì´ê¸°ë„ í•˜ê³ ìš”. íŠ¹íˆ ëª¨ë¸ ê²½ëŸ‰í™” ë“± ì–´ëŠ í•œí¸ì—ì„œëŠ” Non-linear Activation Functionë§ˆì € Bottleneckìœ¼ë¡œ ì§šê³  ë„˜ì–´ê°€ëŠ” ì‹¤ì •ì´ê¸°ì—[<sup>[3]</sup>](#r3), ëª¨ë“  ëª©ì ì„ ë§Œì¡±ì‹œí‚¬ë§Œí•œ ìƒˆë¡œìš´ í™œì„± í•¨ìˆ˜ë¥¼ ì°¾ì€ ì—°êµ¬ëŠ” ì•„ë‹™ë‹ˆë‹¤. ë‹¤ë§Œ, ì‹ì— ëŒ€í•œ ê°„ë‹¨í•œ ì •ë¦¬ë¡œ ReLUì™€ Swish ê°„ì˜ ê´€ê³„ë¥¼ ë³´ì„ê³¼ ë™ì‹œì—, ìƒˆë¡œìš´ Activation Familyë¥¼ ì œì‹œí–ˆë‹¤ëŠ” ë°ì— ì˜ì˜ê°€ ìˆëŠ” ë…¼ë¬¸ì´ì—ˆìŠµë‹ˆë‹¤.

CVPR 2021ì—ì„œ ì´ëŸ¬í•œ ë…¼ë¬¸ë„ ë°œí‘œëœë‹¤ëŠ” ê²ƒì„ í•¨ê»˜ ê³µìœ í•˜ê³  ì‹¶ì–´ ê°„ëµí•˜ê²Œë‚˜ë§ˆ ë¦¬ë·°ë¥¼ ì§„í–‰í•´ë´¤ìŠµë‹ˆë‹¤ :+1:


### References (+ í•¨ê»˜ ì½ìœ¼ë©´ ì¢‹ì€ ë…¼ë¬¸ë“¤)
1. <a name="r1"></a>Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. "Searching for activation functions." arXiv preprint arXiv:1710.05941 (2017). [[paper]][a1] 
2. <a name="r2"></a>Goodfellow, Ian, et al. "Maxout networks." International conference on machine learning. PMLR, 2013. [[paper]][a2]
3. <a name="r3"></a>Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. "TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning
." Part of Advances in Neural Information Processing Systems 33 (NeurIPS 2020) [[paper]][a3]


[a1]: https://arxiv.org/abs/1710.05941
[a2]: http://proceedings.mlr.press/v28/goodfellow13.html
[a3]: https://proceedings.neurips.cc//paper_files/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html