<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="maum.ai BRAIN Team RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="maum.ai BRAIN Team Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-204903244-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech | maum.ai BRAIN Team</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://maum-ai.github.io//blog/sane-tts"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech | maum.ai BRAIN Team"><meta data-rh="true" name="description" content="안정적이고 자연스러운 다국어 TTS 모델인 SANE-TTS를 소개합니다."><meta data-rh="true" property="og:description" content="안정적이고 자연스러운 다국어 TTS 모델인 SANE-TTS를 소개합니다."><meta data-rh="true" property="og:image" content="https://maum-ai.github.io//img/mindslab_default.png"><meta data-rh="true" name="twitter:image" content="https://maum-ai.github.io//img/mindslab_default.png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2022-09-02T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/chohyunjae1,https://github.com/Wonbin-Jung,https://github.com/junjun3518,https://github.com/tonyswoo"><meta data-rh="true" property="article:tag" content="publication,paper-review"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://maum-ai.github.io//blog/sane-tts"><link data-rh="true" rel="alternate" href="https://maum-ai.github.io//blog/sane-tts" hreflang="en"><link data-rh="true" rel="alternate" href="https://maum-ai.github.io//blog/sane-tts" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.def04243.css">
<link rel="preload" href="/assets/js/runtime~main.73344f1d.js" as="script">
<link rel="preload" href="/assets/js/main.3c670f66.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/mindslab_brain.svg" alt="maum.ai BRAIN Team" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/mindslab_brain.svg" alt="maum.ai BRAIN Team" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title"></b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/open-source">Open-Source</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/careers">Careers</a><a class="navbar__item navbar__link" href="/internship">Internship</a><a href="https://github.com/maum-ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">All posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/birp">BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/cbits">CBITS: CryptoBERT Incorporated Trading System</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/nu-wave2">NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates</a></li><li class="sidebarItem_CF0Q"><a aria-current="page" class="sidebarItemLink_miNk sidebarItemLinkActive_RRTD" href="/blog/sane-tts">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/assem-vc">Assem-VC and Assem-Singer:</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/ces-2022-review">CES 2022에 참석한 마인즈랩, AI Human</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/vits">VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/acon">Activate or Not: Learning Customized Activation</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/nu-wave">NU-Wave(Interspeech):</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_rzP5" itemprop="headline">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech</h1><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-09-02T00:00:00.000Z" itemprop="datePublished">September 2, 2022</time> · <!-- -->23 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/chohyunjae1" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/chohyunjae1.png" alt="Hyunjae Cho"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/chohyunjae1" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Hyunjae Cho</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio)</small></div></div></div><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/Wonbin-Jung" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/Wonbin-Jung.png" alt="Wonbin Jung"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/Wonbin-Jung" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Wonbin Jung</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio)</small></div></div></div><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/junjun3518.png" alt="Junhyeok Lee"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Junhyeok Lee</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio, Head)</small></div></div></div><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/tonyswoo" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/tonyswoo.png" alt="Sang Hoon Woo"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/tonyswoo" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sang Hoon Woo</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div id="post-content" class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/abs/2206.12132" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2206.12132-brightgreen.svg?style=flat-square" alt="arXiv"></a>
<a href="https://mindslab-ai.github.io/sane-tts/" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Audio%20Samples&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Hyunjae Cho, Wonbin Jung, Junhyeok Lee, and Sang Hoon Woo. &quot;SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech.&quot; </p><p>INTERSPEECH 2022</p></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="sane-tts-stable-and-natural-end-to-end-multilingual-text-to-speech">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech<a class="hash-link" href="#sane-tts-stable-and-natural-end-to-end-multilingual-text-to-speech" title="Direct link to heading">​</a></h2><p>안녕하세요. MINDsLab Brain팀에서 음성합성 연구를 하고 있는 조현재입니다.</p><p>오늘은 저희 Brain팀에서 같이 쓴 SANE-TTS paper가 <a href="https://www.interspeech2022.org" target="_blank" rel="noopener noreferrer">INTERSPEECH 2022</a>에 accept되었다는 좋은 소식과 함께 소개해드리는 시간을 가지려 합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="contributions">Contributions<a class="hash-link" href="#contributions" title="Direct link to heading">​</a></h3><ul><li>SANE-TTS는 안정적이고 자연스러운 다국어 음성을 합성합니다.</li><li>이 논문에서 제안한 <em>speaker regularization loss</em>는 기존 다국어 TTS 모델에서도 사용된 방법인 domain adversarial training과 비슷한 수준의 퀄리티 향상을 이끌어냅니다.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="introductions">Introductions<a class="hash-link" href="#introductions" title="Direct link to heading">​</a></h3><p>다국어(Multilingual) TTS 모델을 훈련시키는 가장 간단한 방법은 무엇일까요?
당장 머리에 떠오를 만한 방식은, 모델이 음성 합성을 지원할 모든 화자로부터 모델이 사용할 모든 언어에 대해 음성 데이터를 모아 만든 다국어 음성 데이터셋을 사용하는 방식일 겁니다.
그러나 이런 방식은 현실적으로 불가능한데, 왜냐하면 이 방식이 가능하려면 우선 모든 화자가 여러 언어를 유창하게 구사할 줄 알아야 하고, 화자가 한 명 늘어날 때마다 모든 언어에 대해 데이터를 모아야 해서 상당한 비용 부담이 발생하기 때문입니다.
그래서 현실적으로 가능한 방식은 서로 다른 화자들이 녹음한 단일언어 데이터셋을 언어별로 모아 훈련용 데이터셋을 구성하는 것입니다.
예를 들어, 화자 A의 한국어 음성 데이터와 화자 B의 영어 음성 데이터를 모아 쓰는 것이죠.
하지만 이 경우 화자가 데이터셋 내에서 사용하지 않은 언어로도 다국어 TTS가 발화가 가능해야 ‘모든 화자에 대해 학습에 사용된 모든 언어로 음성 합성을 한다는 목표’를 달성할 수 있습니다.
쉽게 말하자면 한국어 화자 A의 목소리로도 자연스럽게 영어 음성을 합성할 수 있어야 합니다.
이렇게 훈련 데이터셋에서 특정 화자가 사용한 언어와 다른 언어로 음성을 합성하는 것을 cross-lingual speech synthesis라 부르며, 위와 같은 이유로 다국어 TTS 모델을 구현하기 위해 cross-lingual speech synthesis를 자연스럽고 안정적으로 만드는 일은 매우 중요합니다.</p><p>지금까지 제안된 기존 다국어 TTS 모델들은 대부분 Tacotron<a href="#r1"><sup>[1]</sup></a> 기반 모델입니다.
하지만 Tacotron 기반 다국어 TTS 모델들은 자기회귀적(autoregressive)으로 텍스트와 음성 사이의 alignment를 계산하기 때문에 문제가 발생할 수 있습니다.<a href="#r2"><sup>[2]</sup></a>
우선 자기회귀적 모델은 작은 attention error에도 민감하게 반응하여 틀린 alignment를 계산하고 이는 음성 합성 시 단어를 반복 혹은 누락하는 문제로 이어집니다.
또한, 자기회귀적으로 attention을 생성하다 보면 발화 길이의 직접적인 제어가 어려워져 부자연스러운 속도로 발화할 가능성도 있습니다.
이러한 문제들을 피하기 위해, 저희는 비자기회귀적 모델들 중 VITS<a href="#r3"><sup>[3]</sup></a>를 기반으로 하여 SANE-TTS를 설계했습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="vits">VITS<a class="hash-link" href="#vits" title="Direct link to heading">​</a></h3><table><thead><tr><th align="center">VITS at training</th><th align="center">VITS at inference</th></tr></thead><tbody><tr><td align="center"><img class="figCenter_mKJ5" src="/assets/images/vits_training-be114c67c6e33a455beda03613a80d9d.png" alt="vits_training"></td><td align="center"><img class="figCenter_mKJ5" src="/assets/images/vits_inference-40f9317e46270a281f19c6af5d6743b3.png" alt="vits_inference"></td></tr></tbody></table><p>전체적인 VITS의 구조는 linear spectrogram을 posterior encoder를 이용해 latent <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span>로 변환한 다음 다시 decoder를 통해 음성을 생성하는 VAE 구조입니다.
이 때 text encoder를 통해 나온 결과값과 latent <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span>를 flow 모듈에 넣고 나온 결과값을 이용해 input text와 target speech간의 alignment를 계산합니다.
이 과정에서 monotonic alignment search가 사용되며 나온 duration 정보를 통해 duration predictor를 훈련시킵니다.</p><p>Inference 과정에서는 text encoder에서 나온 결과값을 duration predictor가 예측한 duration을 통해 speech 길이에 맞게 늘려줍니다.
그 다음 flow reverse 과정과 decoder를 거쳐 speech를 합성합니다.</p><p>VITS 모델은 non-autoregressive한 모델이자 하나의 모델로 text를 speech로 만드는 end-to-end 모델입니다.
VITS에 대한 더 자세한 내용은 <strong><a href="https://mindslab-ai.github.io/blog/vits" target="_blank" rel="noopener noreferrer">VITS post</a></strong>에서 보실 수 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="sane-tts">SANE-TTS<a class="hash-link" href="#sane-tts" title="Direct link to heading">​</a></h3><table><thead><tr><th align="center">SANE-TTS at training</th></tr></thead><tbody><tr><td align="center"><img class="figCenter_mKJ5 medium_PfCl" src="/assets/images/sane_tts_training-e91483d83996efbb156e946a812af65f.png" alt="sane_tts_training"></td></tr></tbody></table><h4 class="anchor anchorWithStickyNavbar_mojV" id="language-embedding">Language embedding<a class="hash-link" href="#language-embedding" title="Direct link to heading">​</a></h4><p>다국어 TTS 모델의 경우에는 여러 언어를 사용하기 때문에 SANE-TTS는 language embedding을 이용해 언어를 구별합니다.
특히 text representation을 직접 받는 text encoder와 duration predictor 모듈에게만 language embedding을 제공하여 각 언어를 구별할 수 있도록 도와줍니다.
Duration predictor에서는 기본 VITS 세팅에서의 speaker embedding과 동일하게 language embedding 역시 convolution layer에 통과시킨뒤 text representation에 더하는 식으로 언어 정보를 모듈에 줍니다.</p><p>Text encoder에서는 Nekvinda and Dusek<a href="#r4"><sup>[4]</sup></a>가 제안한 parameter generation을 사용해 언어 정보를 전달합니다.
이는 하나의 text encoder 모듈로 다양한 언어에 맞는 text representation을 생성하기 위한 방안으로, parameter generator가 language embedding을 받아 언어 정보가 들어있는 parameter를 생성한 다음, parameter들을 text encoder의 각 레이어에 더하는 방식입니다.
Text encoder의 여러 부분에 언어 정보가 제공되었기 때문에 각 언어에 최적화된 text representation을 만들게 됩니다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="domain-adversarial-training">Domain adversarial training<a class="hash-link" href="#domain-adversarial-training" title="Direct link to heading">​</a></h4><p>훈련 데이터에서 화자마다 사용하는 text가 다르기 때문에 모델을 훈련하는 과정에서 text 정보와 화자 정보가 서로 영향을 줄 수 있습니다.
이로 인해 발생할 수 있는 편향문제를 막기 위해 기존 다국어 TTS 모델들<a href="#r4"><sup>[4]</sup></a><a href="#r5"><sup>[5]</sup></a>은 domain adversarial traing(DAT)<a href="#r6"><sup>[6]</sup></a>을 사용했습니다.
SANE-TTS 역시 DAT를 적용하여 위의 문제를 해결했습니다.
SANE-TTS에 대한 DAT 적용 방식은 아래와 같습니다.</p><p>먼저, gradient reversal layer와 fully connected layer로 이루어진 speaker classifer에 text encoder의 결과값인 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">h_\mathrm{text}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">text</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>를 넘겨줍니다.
Gradient reversarial layer는 흐르는 역전파 gradient의 부호를 반대로 바꾸는 역할을 수행합니다.
그 다음 speaker classifier를 화자 분류의 정확도를 측정하는 cross-entropy loss를 통해 훈련하게 된다면, 오히려 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">h_\mathrm{text}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">text</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>는 화자 분류의 정확도가 낮아지도록 화자 정보가 없어지는 방향으로 훈련이 진행됩니다.
따라서 DAT를 적용하면 text encoder는 화자에 독립적으로 학습하여 편향 문제를 해결합니다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="speaker-regularization-loss">Speaker regularization loss<a class="hash-link" href="#speaker-regularization-loss" title="Direct link to heading">​</a></h4><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>L</mi><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">g</mi></mrow></msub><mo>=</mo><mo stretchy="false">∥</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>k</mi><mo>∈</mo><mi>K</mi></mrow></msub><mrow><mo fence="true">[</mo><mrow><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">v</mi></mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo fence="true">]</mo></mrow><msub><mo stretchy="false">∥</mo><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_\mathrm{reg} = \lVert\mathbb{E}_{k \in K}\left[\mathrm{conv}(S_{k})\right]\lVert_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight" style="margin-right:0.01389em">reg</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">∥</span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">[</span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em">conv</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">]</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mopen"><span class="mopen">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span></div><p>다국어 TTS 모델 훈련 데이터에서 보통 한 화자는 단일언어에 대해서만 음성 데이터가 있기 때문에 화자 정보가 언어에 편향될 수 있습니다.
특히 duration predictor에서는 화자 정보와 언어 정보가 동시에 필요하기 때문에 화자 정보의 언어 편향이 큰 문제를 발생시킬 수 있습니다.
이에 SANE-TTS는 duration predictor 내 화자 정보의 언어 편향 문제를 해결하기 위해 <em>speaker regularization loss</em>를 추가합니다.
Speaker regularization loss의 정확한 계산식은 위와 같습니다.
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span>는 하나의 batch를 의미하며 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">S_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>는 datapoint <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span>에서 화자의 speaker embedding을 의미합니다.
또한 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">v</mi></mrow><annotation encoding="application/x-tex">\mathrm{conv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em">conv</span></span></span></span></span></span>는 kernel size가 1인 convolution layer를 의미합니다.
이러한 loss를 통해 hidden speaker representation인 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">v</mi></mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{conv}(S_{k})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em">conv</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>는 평균값이 언어에 관계없이 영벡터에 가까워지도록 압력을 받게 되고 이에 따라 화자 정보는 언어와 분리됩니다.
(Speaker regularization loss의 자세한 역할 설명은 Results의 Figure 4에 나와있습니다.)</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="deterministic-duration-predictor">Deterministic duration predictor<a class="hash-link" href="#deterministic-duration-predictor" title="Direct link to heading">​</a></h4><p>기존 VITS에서는 다양한 duration을 확률적으로 예측하는 stochastic duration predictor(SDP)를 사용했습니다.
하지만 Casanova et al.<a href="#r7"><sup>[7]</sup></a>는 &#x27;여러 언어를 다뤄야 하는 상황에서는 SDP가 때때로 부자연스러운 duration을 예측한다.&#x27;라고 언급한 바 있습니다.
다국어 TTS 모델의 경우 cross-lingual speech synthesis의 안정성을 높이는 것이 중요하기 때문에 저희 SANE-TTS에서는 SDP를 deterministic duration predictor(DDP)로 교체합니다.</p><table><thead><tr><th align="center">SANE-TTS at inference</th></tr></thead><tbody><tr><td align="center"><img class="figCenter_mKJ5 medium_PfCl" src="/assets/images/sane_tts_inference-95b0f53d90fe4bf5a2d8b002f05eacfc.png" alt="sane_tts_inference"></td></tr></tbody></table><h4 class="anchor anchorWithStickyNavbar_mojV" id="inference-procedure">Inference procedure<a class="hash-link" href="#inference-procedure" title="Direct link to heading">​</a></h4><p>영어 화자가 한국어 음성을 말할 때 기본적인 톤, 세기 등은 어느정도 예측이 가능하지만 발화의 duration은 예측하기 힘듭니다.
그 이유는 톤, 세기 같은 특성들은 화자 자체의 특성에 가까워 언어와 관련성이 덜하지만 발화의 duration은 언어와 크게 맞닿아 있기 때문입니다.
따라서 다국어 TTS 모델에서 영어 화자의 특성을 그대로 사용하여 한국어 음소의 duration을 예측한다면 부자연스럽게 들릴 뿐 화자 특성이 반영된다고 보기 어렵습니다.
이러한 문제를 해결하기 위해 저희는 cross-lingual inference 단계에서 약간의 수정을 가했습니다.</p><p>SANE-TTS의 duration predictor에 speaker regularization loss를 추가했기 때문에 hidden speaker representation들은 영벡터 근처에 위치하게 됩니다.
이를 이용해 cross-lingual speech synthesis 과정에서 duration predictor에 speaker embedding 대신 가까운 값인 영벡터를 집어넣습니다.
따라서 SANE-TTS는 cross-lingual speech inference를 할 때 화자가 달라지더라도 duration predictor가 동일한 duration을 예측합니다.
이 방법을 통해 cross-lingual하게 합성한 음성의 안정성을 높이고, duration에 언어가 다른 화자 정보를 반영할때 생기는 모호함을 없앨 수 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="experiments">Experiments<a class="hash-link" href="#experiments" title="Direct link to heading">​</a></h3><img class="figCenter_mKJ5 medium_PfCl" src="/assets/images/dataset-07e2465b8d4d70e1456b082da25d6a29.png" alt="Table1"><p>훈련 데이터셋에는 영어(EN), 한국어(KO), 일본어(JA), 중국어(ZH) 화자가 있습니다.
위 표에 나와있듯이 총 472명의 화자와 203.4시간의 음성을 사용하고 음성 데이터의 5%는 validation에 사용됩니다.</p><p>음성의 품질을 측정하기 위해 MOS 평가를 진행합니다.
MOS 평가는 음성의 품질을 평가하기 위해 사용되는 주관적 기법으로, 평가자들이 음성에 대해 1~5점 사이의 평점을 매겨 평점이 높을수록 성능이 높다고 판단하는 방식입니다.
Metric으로는 naturalness MOS와 similarity MOS를 사용하는데, 각각 naturalness MOS는 합성된 음성이 얼마나 자연스럽게 들리는지, similarity MOS는 합성된 음성의 화자가 원래 음성의 화자와 얼마나 비슷하게 들리는지에 대한 MOS 평가 점수입니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="results">Results<a class="hash-link" href="#results" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_mojV" id="speech-synthesis-quality">Speech synthesis quality<a class="hash-link" href="#speech-synthesis-quality" title="Direct link to heading">​</a></h4><img class="figCenter_mKJ5 medium_PfCl" src="/assets/images/compare-480c0a1e5f51943495cfd3ca3c590846.png" alt="Table23"><p>SANE-TTS와 다른 다국어 TTS model인 Meta-learning model<a href="#r4"><sup>[4]</sup></a><a href="#r8"><sup>[8]</sup></a>를 비교한 결과 음성의 자연스러움(naturalness MOS)과 화자의 유사성(similarity MOS) 모두 SANE-TTS가 더 좋은 결과를 냈습니다.
특히 cross-lingual speech synthesis 시 SANE-TTS는 화자에 관계없이 일정한 duration을 예측하기 때문에 안정적인 음성을 합성할 수 있습니다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="ablation-study">Ablation study<a class="hash-link" href="#ablation-study" title="Direct link to heading">​</a></h4><img class="figCenter_mKJ5 medium_PfCl" src="/assets/images/ablation-ab575174179db86bee61ec0433188f2c.png" alt="Table45"><p>DAT를 적용하는 것과 speaker regularization loss를 추가하는 것 모두 음성의 자연스러움과 화자의 유사성을 향상시킵니다.
반면에 SDP를 사용하는 것과 기본인 DDP를 사용하는 것에는 통계상으로 유의미한 차이가 보이지 않습니다.
하지만 SDP는 종종 부자연스러운 duration을 생성한다는 문제점이 존재합니다.
Figure 3에 나와있는 예시를 보시면 SDP가 잘못된 duration을 생성하여 합성된 음성의 중간에 긴 침묵이 발생합니다.
이런 잘못된 경우를 방지하기 위해 SANE-TTS는 비교적 안정적으로 duration을 생성하는 DDP를 사용합니다.</p><table><thead><tr><th align="left">Figure 3: DDP와 SDP를 사용한 모델이 생성한 alignments와 mel spectrograms figure입니다. 사용한 text는 “What was the good of living, and why should he live now?”이고, 화자는 KSS입니다. DDP를 사용한 모델은 정상적인 (a) alignment와 (b) mel spectrogram을 생성한 반면, SDP를 사용한 모델은 때때로 부자연스러운 (c) alignment와 (d) mel spectrogram을 생성하고 음성 중간에 긴 침묵이 생깁니다.</th></tr></thead><tbody><tr><td align="left"><img class="figCenter_mKJ5 small_g5vo" src="/assets/images/SDP-d303e64683dcbb69fe324b8f572c0995.png" alt="SDP"></td></tr></tbody></table><p>아래에 있는 Figure 4는 duration predictor의 hidden speaker representation을 t-SNE plot으로 그린 결과입니다.
Speaker regularization loss를 사용하지 않은 (b)의 경우에는 같은 언어끼리 화자 정보가 뭉치게 되지만, speaker regularization loss를 추가한 (a)의 경우 언어에 상관없이 영벡터를 중심으로 분포하고 있습니다.
이 t-SNE plot은 다국어 TTS 모델에 speaker regularization loss를 추가하면 화자 정보가 언어와 분리되고 영벡터가 hidden speaker representation의 대표 역할을 할 수 있음을 보여주고 있습니다.</p><table><thead><tr><th align="left">Figure 4: (a) Speaker regularization loss 를 추가했을 때, (b) speaker regularization loss를 추가하지 않았을 때 472명의 모든 화자에 대한 duration predictor의 hidden speaker representation을 t-SNE plot으로 그린 figure입니다.</th></tr></thead><tbody><tr><td align="left"><img class="figCenter_mKJ5 medium_PfCl" src="/assets/images/t_SNE-ee4a0929a2c5f43ec80517c974e28eeb.png" alt="t-SNE"></td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_mojV" id="audio-samples">Audio samples<a class="hash-link" href="#audio-samples" title="Direct link to heading">​</a></h3><p>아래에 있는 표에는 SANE-TTS를 통해 합성한 다국어 음성들이 있습니다.
다양한 화자들이 4개 언어(영어, 한국어, 일본어, 중국어)의 text target을 읽고 있으며, 읽는 화자로는 영어 데이터셋에 있는 LJ와 LibriTTS4356, 한국어 데이터셋에 있는 KSS, 일본어 데이터셋에 있는 jvs001 그리고 중국어 데이터셋에 있는 SSB0073을 선택했습니다.</p><table><thead><tr><th align="center">Speaker</th><th align="center">Reference Speech</th><th align="center">English Target</th><th align="center">Korean Target</th><th align="center">Japanese Target</th><th align="center">Chinese Target</th></tr></thead><tbody><tr><td align="center">LJ</td><td align="center"><audio controls=""> <source src="/assets/medias/gt_1-f41d1debacc23fcfac3579b18765232d.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/en_1-a4fc578f46af5cc96f9d200407a1281f.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/ko_1-e9193194356764ae70c942eddc173452.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/ja_1-e51fa3cbefcaf513c03c080d5593b933.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/zh_1-9813b8508217d8963ed0a8be39b25d8c.wav"> </audio></td></tr><tr><td align="center">LibriTTS<br>4356</td><td align="center"><audio controls=""> <source src="/assets/medias/gt_2-d09eac981cee064baae3d9abfe44f88f.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/en_2-a2d11e16cf248161a789be144e1fe1e1.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/ko_2-492591494dfda184346332049d812fa2.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/ja_2-f7aea54be58d5524107511208e9d869f.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/zh_2-c1dcda178aaa2f26da61b19d9b9f83b1.wav"> </audio></td></tr><tr><td align="center">KSS</td><td align="center"><audio controls=""> <source src="/assets/medias/gt_3-d22268dc653f21e2eeb0ad6bff07c9f4.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/en_3-3233d9967947afd600eb4531c5e6a9d6.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/ko_3-15c710129144707b14b2eaa155e2fe2f.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/ja_3-6592da5f74d1525b27b39b6dce59dbf0.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/zh_3-c08c7cb2e04cf81f82edb032028ab8cc.wav"> </audio></td></tr><tr><td align="center">jvs001</td><td align="center"><audio controls=""> <source src="/assets/medias/gt_4-617cadc637503dddbaf66134ab30e214.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/en_4-dc449ec6e0be46f34f9a094afb92dd2e.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/ko_4-beef8090fb48858ee066fc0600d1a4d0.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/ja_4-2a6b96dafcead8f7667145c6105bcf29.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/zh_4-352208138ae680ba0453dcd524eb2b57.wav"> </audio></td></tr><tr><td align="center">SSB0073</td><td align="center"><audio controls=""> <source src="/assets/medias/gt_5-aea3a5dc62e4c1e6f0d1bd24a94119ce.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/en_5-2ec2acaea35aae6687b47b0f1c46bacb.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/ko_5-34416f3fa4d06c50c604204c0518f589.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/ja_5-5fca73dcf05bc7a924f74075bd30c639.wav"> </audio></td><td align="center"><audio controls=""> <source src="/assets/medias/zh_5-878b47df96aa48fa4a51169f8ec49972.wav"> </audio></td></tr></tbody></table><p><strong>English Target</strong>: I saw him come from your window, and I saw all that passed between you in the balcony.<br>
<strong>Korean Target (Romanization)</strong>: 언젠가 들은 적이 있는 것 같거든요. (Eonjenga deul-eun jeog-i issneun geos gatgeodeun-yo.)<br>
<strong>Japanese Target (Romanization)</strong>: 許可書がなければここへは入れない。(Kyoka-sho ga nakereba koko e wa hairenai.)<br>
<strong>Chinese Target (Hanyu Pinyin)</strong>: 替我播放相思风雨中 (Tì wǒ bòfàng xiāngsī fēngyǔ zhōng)  </p><p><strong><a href="https://mindslab-ai.github.io/sane-tts/" target="_blank" rel="noopener noreferrer">Github sample page</a></strong>에는 SANE-TTS로 합성한 기본적인 다국어 음성을 포함하여 다른 다국어 TTS 모델과의 비교, ablation study 음성 샘플들이 있습니다.
약 500단어의 긴 문장을 자연스럽게 발화하거나, 논문에는 나와있지 않지만 code-switching이 적용된 음성 샘플들도 들을 수 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusions">Conclusions<a class="hash-link" href="#conclusions" title="Direct link to heading">​</a></h3><p>저희 논문에서는 speaker regularization loss과 함께 cross-lingual speech synthesis 과정에서 speaker embedding 대신 영벡터를 집어넣는 방식을 제안했고, 그 결과 안정적이고 자연스러운 음성을 합성하는 다국어 TTS 모델인 SANE-TTS를 만들었습니다.</p><p><strong>이상으로 긴 글 읽어주셔서 대단히 감사합니다.</strong>
<strong>INTERSPEECH 2022에서 만나요!</strong></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h3><a name="r1"></a><ol><li>J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan, R. A. Saurous, Y. Agiomvrgiannakis, and Y. Wu, “Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 4779–4783. <a href="https://arxiv.org/abs/1712.05884" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><a name="r2"></a><ol start="2"><li>Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, “FastSpeech: Fast, Robust and Controllable Text to Speech,” in Advances in Neural Information Processing Systems, vol. 32, 2019. <a href="https://arxiv.org/abs/1905.09263" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><a name="r3"></a><ol start="3"><li>J. Kim, J. Kong, and J. Son, “Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech,” in International Conference on Machine Learning, vol. 139, 2021, pp.5530–5540. <a href="https://arxiv.org/abs/2106.06103" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><a name="r4"></a><ol start="4"><li>T. Nekvinda and O. Dusek, “One Model, Many Languages: Meta-Learning for Multilingual Text-to-Speech,” in INTERSPEECH, 2020, pp. 2972–2976. <a href="https://arxiv.org/abs/2008.00768" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><a name="r5"></a><ol start="5"><li>Y. Zhang, R. J. Weiss, H. Zen, Y. Wu, Z. Chen, R. Skerry-Ryan, Y. Jia, A. Rosenberg, and B. Ramabhadran, “Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning,” in INTERSPEECH, 2019, pp. 2080–2084. <a href="https://arxiv.org/abs/1907.04448" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><a name="r6"></a><ol start="6"><li>Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. March, and V. Lempitsky, “Domain-Adversarial Training of Neural Networks,” Journal of Machine Learning Research, vol. 17, no. 59, pp. 1–35, 2016. <a href="https://arxiv.org/abs/1505.07818" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><a name="r7"></a><ol start="7"><li>E. Casanova, J. Weber, C. Shulby, A. C. Junior, E. Golge, and M. A. Ponti, “YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone,” arXiv preprint arXiv:2112.02418, 2021. <a href="https://arxiv.org/abs/2112.02418" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><a name="r8"></a><ol start="8"><li>Official source code of Meta-learning model: <a href="https://github.com/Tomiinek/Multilingual_Text_to_Speech" target="_blank" rel="noopener noreferrer">[github]</a></li></ol></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_h6_j"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/publication">publication</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/paper-review">paper-review</a></li></ul></div></footer></article><div></div><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/nu-wave2"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/assem-vc"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Assem-VC and Assem-Singer:</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_cNA8 thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#sane-tts-stable-and-natural-end-to-end-multilingual-text-to-speech" class="table-of-contents__link toc-highlight">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech</a><ul><li><a href="#contributions" class="table-of-contents__link toc-highlight">Contributions</a></li><li><a href="#introductions" class="table-of-contents__link toc-highlight">Introductions</a></li><li><a href="#vits" class="table-of-contents__link toc-highlight">VITS</a></li><li><a href="#sane-tts" class="table-of-contents__link toc-highlight">SANE-TTS</a></li><li><a href="#experiments" class="table-of-contents__link toc-highlight">Experiments</a></li><li><a href="#results" class="table-of-contents__link toc-highlight">Results</a></li><li><a href="#audio-samples" class="table-of-contents__link toc-highlight">Audio samples</a></li><li><a href="#conclusions" class="table-of-contents__link toc-highlight">Conclusions</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Contents</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/publications">Publications</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/maum-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/careers">Careers</a></li><li class="footer__item"><a class="footer__link-item" href="/blog/tags">Tags</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 maum.ai BRAIN Team. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.73344f1d.js"></script>
<script src="/assets/js/main.3c670f66.js"></script>
</body>
</html>