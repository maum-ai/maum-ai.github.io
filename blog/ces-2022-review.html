<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="maum.ai BRAIN Team RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="maum.ai BRAIN Team Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-204903244-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">CES 2022에 참석한 마인즈랩, AI Human | maum.ai BRAIN Team</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://maum-ai.github.io//blog/ces-2022-review"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="CES 2022에 참석한 마인즈랩, AI Human | maum.ai BRAIN Team"><meta data-rh="true" name="description" content="Brain팀을 대표하여 CES 2022 참석한 후기를 전달드리고, AI Human 관련한 Brain팀의 연구를 소개합니다."><meta data-rh="true" property="og:description" content="Brain팀을 대표하여 CES 2022 참석한 후기를 전달드리고, AI Human 관련한 Brain팀의 연구를 소개합니다."><meta data-rh="true" property="og:image" content="https://maum-ai.github.io//img/mindslab_default.png"><meta data-rh="true" name="twitter:image" content="https://maum-ai.github.io//img/mindslab_default.png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2022-02-17T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/deepkyu"><meta data-rh="true" property="article:tag" content="news"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://maum-ai.github.io//blog/ces-2022-review"><link data-rh="true" rel="alternate" href="https://maum-ai.github.io//blog/ces-2022-review" hreflang="en"><link data-rh="true" rel="alternate" href="https://maum-ai.github.io//blog/ces-2022-review" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.def04243.css">
<link rel="preload" href="/assets/js/runtime~main.396e3776.js" as="script">
<link rel="preload" href="/assets/js/main.e6055739.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/mindslab_brain.svg" alt="maum.ai BRAIN Team" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/mindslab_brain.svg" alt="maum.ai BRAIN Team" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title"></b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/open-source">Open-Source</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/careers">Careers</a><a href="https://github.com/maum-ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">All posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/birp">BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/cbits">CBITS: CryptoBERT Incorporated Trading System</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/nu-wave2">NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/sane-tts">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/assem-vc">Assem-VC and Assem-Singer:</a></li><li class="sidebarItem_CF0Q"><a aria-current="page" class="sidebarItemLink_miNk sidebarItemLinkActive_RRTD" href="/blog/ces-2022-review">CES 2022에 참석한 마인즈랩, AI Human</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/vits">VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/acon">Activate or Not: Learning Customized Activation</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/nu-wave">NU-Wave(Interspeech):</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_rzP5" itemprop="headline">CES 2022에 참석한 마인즈랩, AI Human</h1><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-02-17T00:00:00.000Z" itemprop="datePublished">February 17, 2022</time> · <!-- -->18 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/deepkyu.png" alt="Hyoung-Kyu Song"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Hyoung-Kyu Song</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Vision, Head)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div id="post-content" class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_mojV" id="brain-team-at-ces-2022">Brain Team at CES 2022<a class="hash-link" href="#brain-team-at-ces-2022" title="Direct link to heading">​</a></h2><p>안녕하세요! 마인즈랩 Brain팀 Vision 파트 연구원 송형규입니다.<br>
<!-- -->지난 1월 5일부터 7일까지 3일동안 미국 라스베가스에서 세계 최고 가전 박람회인 CES 2022가 열렸습니다. 2년만에 오프라인으로 다시 돌아온 이번 CES 2022에는 오미크론이 확산되면서 평년 대비 많은 인원이 참석하지는 않았지만, 워낙 큰 박람회여서 그런지 그 규모는 대단했습니다.<br>
<!-- -->마인즈랩은 이번 CES 2022에서 국내 은행과 함께 구현하여 현재 상용에 도입된 AI Banker를 전시했습니다. 저는 이번에 기술 설명을 위해 CES 출장길에 동행했는데요. 박람회인만큼 연구자들이 중심이 되는 딥러닝/머신러닝 학회와는 완전히 다른 분위기지만, 투자자들과 바이어, 일반 참석자들이 참석하는 자리이다 보니까 매의 눈으로 기능 하나하나에 대해 여쭤보시는 분들이 더욱 많아 개인적으로 논문 발표하는 것보다 더 어려웠습니다(다녀오니까 살이 2kg 빠졌어요 ㅎㅎ;).<br>
<!-- -->이번 포스트에서는 CES 2022에서의 분위기를 간접적으로나마 전달드리고, 이번에 AI Human에 도입된 기술들에 대해 설명드리겠습니다. 이번 글은 기술 블로그이기는 하나, 기술적인 내용을 다루기보다는 딥러닝으로 해결한 Task가 어떤 것이었는지를 말씀드리려고 합니다 😄</p><img class="figCenter_mKJ5" src="/assets/images/figure1_vegas-0be5f563e9f3989ba0f8c04d02eff756.jpeg" alt="figure1_vegas"><img class="figCenter_mKJ5" src="/assets/images/figure2_ces-880bc686b4e86feb756f738482f1b5de.png" alt="figure2_ces"><h3 class="anchor anchorWithStickyNavbar_mojV" id="ces-2022">CES 2022<a class="hash-link" href="#ces-2022" title="Direct link to heading">​</a></h3><p>CES는 라스베가스 전역에 걸쳐 3개의 전시관(Tech East, Tech West, Tech South)에서 진행됩니다. 그 중 대규모 부스가 주로 자리잡는 Tech East만 해도 코엑스 전시관 규모의 3배 이상의 크기를 자랑합니다. 특히, 이번에 공개한 <a href="https://www.boringcompany.com/vegas-loop" target="_blank" rel="noopener noreferrer">Vegas Loop</a>도 Tech East 내의 West Hall과 Central Hall을 이어주는데, 그만큼 단일 엑스포 자체도 차를 타고 다녀도 될 정도로 그 규모가 큽니다. 엑스포 각각에 전시한 것들이 모두 의미가 있지만, 아무래도 Tech East와 Tech West의 전시관 규모가 큽니다. 이번 CES 2022에서 마인즈랩은 Tech West의 Eureka Hall에 자리 잡았습니다.</p><img class="figCenter_mKJ5" src="/assets/images/figure3_booth-2fb46408aed8378bc785456459e48122.png" alt="figure3_booth"><h3 class="anchor anchorWithStickyNavbar_mojV" id="마인즈랩의-ai-banker">마인즈랩의 AI Banker<a class="hash-link" href="#마인즈랩의-ai-banker" title="Direct link to heading">​</a></h3><p>이번 AI Banker는 국내 모 은행에 이미 상용으로 도입이 되어 있는 기기로, 컨시어지 기기와 디지털데스크 두 종류로 구성되어 있습니다. 우리가 보통 은행을 가면 은행 창구 안내와 함께 번호표를 뽑는데요. AI Banker 컨시어지가 번호표 출력과 더불어 해당 지점에 대한 안내 등의 업무를 대신합니다. 그리고 AI Banker가 탑재된 디지털데스크는 은행 창구의 위치에서 실제 은행을 방문하여 하는 업무들을 대신하여 진행할 수 있도록 구성되었습니다. 마인즈랩은 이번 AI Banker에 도입된 인공 인간에 대해 음성 생성(TTS, Text-To-Speech) 기술과 얼굴 생성(STF, Speech-To-Face) 기술을 구현함과 동시에 AI Banker 내 여러 기능들에 대한 개발을 진행했습니다. 이를 통해, 발화할 특정 텍스트가 정해지면, 이를 발화하는 인공 인간 영상을 무궁무진하게 만들어낼 수 있습니다. 특히, AI Banker 컨시어지는 GPU가 내장된 Windows PC에서 동작하는데, TTS와 STF 두 엔진이 서버를 통하지 않고도 Windows PC 내에서 생성될 수 있도록 구현을 진행했고, 10초 영상을 생성하는 데에 5초가 소요되는 속도를 자랑하게 되었습니다.  </p><p>이번 AI Banker는 앞서 말씀드렸듯이 이미 국내 일부 지점에서 상용으로 도입이 되어 있어, 해당 지점에 가셔서 확인하실 수도 있습니다. 인공 인간이 상용에 도입되어 실제 작동하는 사례가 많지 않다보니 이 부분만 해도 이미 CES 2022에서 큰 임팩트를 불러올 법 했는데요. 사실 여기에 더하여 기존 기기에서 확인할 수는 없지만, CES 2022에서 처음으로 공개한 기술이 있습니다.</p><p>바로 영어, 중국어, 일본어를 하지 못하는 사람도 본인의 목소리로 4개국어를 할 수 있게 만드는, 다국어(Multilingual) TTS 및 STF 기술입니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="기술-소개">기술 소개<a class="hash-link" href="#기술-소개" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="multilingual-vits">Multilingual VITS<a class="hash-link" href="#multilingual-vits" title="Direct link to heading">​</a></h3><p>Multilingual TTS 라는 표현은 여러 언어의 음성을 생성하는 기술을 말하는데요. 해당 발화자가 원래 다국어가 가능한 사람이여서 데이터 자체를 여러 언어 음성으로 구성하여 학습시키는 것도 Multilingual TTS에 포함됩니다. 마인즈랩 Multilingual TTS는 <strong>특정 발화자의 한국어 음성 데이터만을 가지고</strong>, 해당 발화자의 목소리 특성을 살려 한국어는 물론, 영어, 중국어, 일본어까지 발화할 수 있게 만드는 기술입니다. 이는 세부적으로 Cross-lingual TTS로도 불리는데, 아직 학계에서도 굉장히 어려운 문제로 꼽고 있습니다.<br>
<!-- -->방식을 조금 더 설명드리자면, 각 언어(한국어, 중국어, 일본어, 영어)에 대해 여러 화자로부터 TTS 데이터를 구축한 다음, 이를 가지고 1차 학습(Baseline)을 진행합니다. 이후 기존 TTS 모델에 Regularization 방법을 가미하여 언어 조건에 맞추어 음성이 합성되도록 추가 학습을 진행합니다. 일반적인 Fine-tuning과 유사하다고 느끼실 수 있으나, 이 과정에서 loss function에 regluarization term이 하나 더 추가된다는 점, 그리고 이를 적용했을 때 여러 언어를 소화할 수 있게끔 robustness를 확보하고 있는 TTS 모델이어야 한다는 점에서 결코 쉽게 찾을 수 있는 방법은 아닙니다. 이 방법을 통해 원래 한국어밖에 하지 못하는 분들도 영어, 일본어, 중국어를 발화하는 영상을 생성할 수 있게 됩니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="speech-to-face-stf">Speech-To-Face (STF)<a class="hash-link" href="#speech-to-face-stf" title="Direct link to heading">​</a></h3><p>CES에 전시한 AI Human에서 보는 얼굴은 순수하게 딥러닝 모델로 생성된 얼굴 그대로입니다. 얼굴 생성 모델은 저를 포함하여 Brain팀 AI Face 프로젝트 연구원분들과 함께 자체적으로 개발한 알고리즘을 기반으로 구축되었습니다.<br>
<!-- -->일반적으로 학계에서는 Talking Head Generation 이라는 이름으로도 Task를 부르곤 합니다. 일반적으로 연구에서 집중하는 부분은 Any-Face, Any-Speaker (Unconstrained) 모델로서, 학습 데이터에 없는 얼굴에 대해 학습 데이터에 없는 톤을 가진 목소리에 대해 영상을 생성하는 것을 소화하려고 합니다. 대신 특정 인물에 대해 학습을 진행하지 않기 때문에, 일반적으로 화질이 낮고(생성하는 영역의 한 변이 256 픽셀보다 적습니다), 입모양 움직임이 부자연스러운 경우가 많습니다. 특히, 여러 논문에서 &quot;영어 데이터로 학습을 해도 어느 언어든 영상을 생성할 수 있다&quot;고 주장하는데요. 실제로 영상을 만들어 보면, 한국어 음성에 대해서는 입모양이 잘 맞지 않는 것을 확인할 수 있습니다.<br>
<!-- -->마인즈랩 얼굴 생성 엔진은 특정 인물의 데이터를 기준으로 학습을 하여 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>×</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">512 \times 512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">512</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">512</span></span></span></span></span> 크기의 얼굴 영상을 생성합니다. 그리고 Multilingual TTS가 있기 때문에, 해당 인물이 한국어뿐만 아니라, 영어, 일본어, 중국어를 발화하는 영상도 만들 수 있게 됩니다. 재밌는 점은 한국어를 기준으로 학습할 경우, 한국어에서 발생하는 발음 조합이 영어보다 다양하고 학습한 데이터 역시 단일 화자의 음성 톤이기 때문에, 다른 언어로 생성한 음성에 대해서도 입모양이 잘 맞습니다.  </p><h3 class="anchor anchorWithStickyNavbar_mojV" id="inference-speed">Inference Speed<a class="hash-link" href="#inference-speed" title="Direct link to heading">​</a></h3><p>딥러닝 기반 생성 모델(Generative Model)을 실제 서비스할 때는 생성하는 음성과 영상의 퀄리티도 중요하지만, 모델에 소모되는 자원과 속도도 중요합니다. 특히, 영상을 생성하는 Task에서는 초당 몇 장(FPS, Frames Per Second)을 생성할 수 있는 지에 따라 그 쓰임새가 달라집니다. 25FPS 영상을 만드는 데 25FPS보다 빠르다면, 이는 실시간 스트리밍도 가능한 수준입니다. 반대로 이보다 느리다면, 영상을 미리 생성해놓고 캐싱해서 사용할 수 밖에 없습니다.<br>
<!-- -->최근 화두가 되는 여러 회사의 인공 인간들을 보면 스트리밍을 지원하는 것들이 많지 않습니다. 미리 생성한 영상을 가져다가 만들곤 하죠. Brain팀이 만들고자 하는 AI Human은 실시간으로 대화가 가능한 모델입니다.<br>
<!-- -->모델에 있어 lightweight 하고 inference 할 때 빠르도록 디자인하는 방법은 여러 가지가 있습니다. Brain팀이 구성한 모델에 대해서는 자세히 말씀드리기 어렵지만, 주로 autoregressive한 요소들을 최대한 non-autoregressive 하도록 변경하는 방향으로 디자인했습니다. 이를 통해 RTX 3080 GPU가 탑재된 데스크탑에서도 음성 생성 및 얼굴 영상 생성까지 54.1FPS 의 속도를 자랑합니다. 이를 통해 AI Human이 방문한 고객의 이름을 읽어주거나, 현장 상황에 알맞는 발화 영상을 생성할 수 있게 구성할 수 있게 됩니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="마무리">마무리<a class="hash-link" href="#마무리" title="Direct link to heading">​</a></h3><p>국내 은행에는 한국어 발화 영상이 구성되어 있었지만, 이번 CES에서는 영어 발화 영상으로 전시가 되었습니다. 타 기업들에서도 그렇고 마인즈랩에서도 영어 TTS와 호환되는 얼굴 생성 엔진이 별도로 존재하기 때문에, 영어 발화 영상을 가지고 전시할 수 있지만, 막상 음성을 들어보면 실제 발화자가 다국어 발화를 할 수 있는 게 아니면 TTS 목소리가 본인 목소리가 아닌 경우가 많습니다. 영어 영상 뿐만 아니라, 동일한 톤과 얼굴로 일본어, 중국어를 발화하는 AI Human을 전시한 덕에 더욱 많은 분들로부터 이목을 끌었습니다.<br>
<!-- -->이번 CES에서 보여드린 다국어 발화 가능한 AI Human 기술들을 통하여 원래 중국어, 영어, 일본어를 하지 못하는 분도 4개 국어를 하실 수 있도록 만들 수 있습니다. 이 외로도 Brain팀은 빠른 추론 속도와 좋은 퀄리티의 얼굴 영상을 기반으로 더욱 자연스럽고 대화가 가능한 AI Human이 되는 데에 필요한 여러 알고리즘을 구성해나갈 예정입니다.  </p><p>CES에서 느낀 재미는 학회를 참석해서 느끼는 감동과 재미는 색달랐습니다. 학회에서는 여러 가지 Contribution에 대한 질문, 방법론에 대한 질문이 많았다면, CES에서 AI Human에 대해 받은 질문들은 품질 자체에 대해 꼼꼼한 피드백들이 주를 이뤘습니다. 예를 들어, 비디오 재생이 느리거나 버벅이는 부분이 기기의 문제인지 알고리즘의 문제인지를 여쭤보는 질문들이었습니다. 또한, 더욱 많은 손동작들이 가능했으면 좋겠다는 피드백이나 밖에서 걸어 들어오고 나가는 등의 기획이 뒷받침되면 좋을 것 같다는 의견도 있었습니다.<br>
<!-- -->보통 딥러닝 모델을 연구하고 만드는 사람이 이러한 피드백을 직접 받는 일이 없는데, 직접 피드백을 들어 보면서 모델에서 수정해야 할 부분이나 개선할 수 있는 부분이 있을 지 생각해보는 시간이 되었습니다. 백엔드나 서비스 기획에서 해결해야 할 이슈 역시 모델을 구성하는 입장에서 좋은 피드백이 되었습니다. 아무래도 생성 모델을 연구하고 있고, 생성한 영상이나 음성이 그대로 사용자에게 노출이 되다 보니 이러한 피드백이 더욱 와닿는 것 같습니다. 아 물론, 저는 그래도 CES보다는 학회가 마음도 편하고 더 재밌는 것 같습니다 😄  </p><img class="figCenter_mKJ5" src="/assets/images/figure4_vegas-09c6b4a3c527df17534a946a0c642837.png" alt="figure4_vegas"><p>Brain팀에서 Publish한 2022년 CVPR, ICASSP Paper를 포함하여, 2021년 NeurIPS Workshop, InterSpeech Paper에도 많은 관심 부탁드립니다!</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="tldr">TL;DR<a class="hash-link" href="#tldr" title="Direct link to heading">​</a></h3><ul><li>마인즈랩 AI Human이 미국 라스베가스에서 열린 CES 2022에 전시되었습니다.</li><li>한국어 데이터만 가지고 영어, 일본어, 중국어까지 발화하는 데모로 정말 많은 호평을 받았습니다.</li><li>학회 말고 전시회에 참석하면서 생성 모델 연구와 생성 모델 상용화 사이의 간극을 한 번 더 느꼈습니다.</li></ul></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_h6_j"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/news">news</a></li></ul></div></footer></article><div></div><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/assem-vc"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Assem-VC and Assem-Singer:</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/vits"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_cNA8 thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#brain-team-at-ces-2022" class="table-of-contents__link toc-highlight">Brain Team at CES 2022</a><ul><li><a href="#ces-2022" class="table-of-contents__link toc-highlight">CES 2022</a></li><li><a href="#마인즈랩의-ai-banker" class="table-of-contents__link toc-highlight">마인즈랩의 AI Banker</a></li></ul></li><li><a href="#기술-소개" class="table-of-contents__link toc-highlight">기술 소개</a><ul><li><a href="#multilingual-vits" class="table-of-contents__link toc-highlight">Multilingual VITS</a></li><li><a href="#speech-to-face-stf" class="table-of-contents__link toc-highlight">Speech-To-Face (STF)</a></li><li><a href="#inference-speed" class="table-of-contents__link toc-highlight">Inference Speed</a></li><li><a href="#마무리" class="table-of-contents__link toc-highlight">마무리</a></li><li><a href="#tldr" class="table-of-contents__link toc-highlight">TL;DR</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Contents</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/publications">Publications</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/maum-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/careers">Careers</a></li><li class="footer__item"><a class="footer__link-item" href="/blog/tags">Tags</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 maum.ai BRAIN Team. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.396e3776.js"></script>
<script src="/assets/js/main.e6055739.js"></script>
</body>
</html>