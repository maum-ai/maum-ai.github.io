<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="maum.ai BRAIN Team RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="maum.ai BRAIN Team Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-204903244-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">NU-Wave(Interspeech): | maum.ai BRAIN Team</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://maum-ai.github.io//blog/nu-wave"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="NU-Wave(Interspeech): | maum.ai BRAIN Team"><meta data-rh="true" name="description" content="최초로 48kHz로 upsampling을 성공한 저희 연구를 소개합니다."><meta data-rh="true" property="og:description" content="최초로 48kHz로 upsampling을 성공한 저희 연구를 소개합니다."><meta data-rh="true" property="og:image" content="https://maum-ai.github.io//img/mindslab_default.png"><meta data-rh="true" name="twitter:image" content="https://maum-ai.github.io//img/mindslab_default.png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2021-07-14T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/junjun3518,https://github.com/Seungwoo0326"><meta data-rh="true" property="article:tag" content="publication,paper-review"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://maum-ai.github.io//blog/nu-wave"><link data-rh="true" rel="alternate" href="https://maum-ai.github.io//blog/nu-wave" hreflang="en"><link data-rh="true" rel="alternate" href="https://maum-ai.github.io//blog/nu-wave" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.def04243.css">
<link rel="preload" href="/assets/js/runtime~main.685fb885.js" as="script">
<link rel="preload" href="/assets/js/main.4fd61293.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/mindslab_brain.svg" alt="maum.ai BRAIN Team" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/mindslab_brain.svg" alt="maum.ai BRAIN Team" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title"></b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/open-source">Open-Source</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/careers">Careers</a><a class="navbar__item navbar__link" href="/internship">Internship</a><a href="https://github.com/maum-ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">All posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/birp">BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/cbits">CBITS: CryptoBERT Incorporated Trading System</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/nu-wave2">NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/sane-tts">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/assem-vc">Assem-VC and Assem-Singer:</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/ces-2022-review">CES 2022에 참석한 마인즈랩, AI Human</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/vits">VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/acon">Activate or Not: Learning Customized Activation</a></li><li class="sidebarItem_CF0Q"><a aria-current="page" class="sidebarItemLink_miNk sidebarItemLinkActive_RRTD" href="/blog/nu-wave">NU-Wave(Interspeech):</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_rzP5" itemprop="headline">NU-Wave(Interspeech):</h1><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-07-14T00:00:00.000Z" itemprop="datePublished">July 14, 2021</time> · <!-- -->16 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/junjun3518.png" alt="Junhyeok Lee"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Junhyeok Lee</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio, Head)</small></div></div></div><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/Seungwoo0326" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/Seungwoo0326.png" alt="Seungu Han"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/Seungwoo0326" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Seungu Han</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div id="post-content" class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/abs/2104.02321" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2104.02321-brightgreen.svg?style=flat-square" alt="arXiv"></a> <a href="https://github.com/mindslab-ai/nuwave" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/mindslab-ai/nuwave?color=yellow&amp;label=NU-Wave&amp;logo=github&amp;style=flat-square" alt="GitHub Repo stars"></a> <a href="https://mindslab-ai.github.io/nuwave/" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-audio_samples-blue?logo=Github&amp;style=flat-square" alt="githubio"></a></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="a-diffusion-probabilistic-model-for-neural-audio-upsampling">A Diffusion Probabilistic Model for Neural Audio Upsampling<a class="hash-link" href="#a-diffusion-probabilistic-model-for-neural-audio-upsampling" title="Direct link to heading">​</a></h2><p>안녕하세요 MINDs Lab Brain에서 Audio와 Speech 연구를 하고 있는 <a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer">이준혁</a>입니다. Audio domain의 딥러닝 연구는 대부분 <em>Sampling Rate</em> 16kHz인 경우에 대해서 진행되었습니다. 생성모델 (TTS) 같은 경우 22.05kHz인 경우도 있었지만 음악이나 영화 쪽에서 많이 쓰이는 44.1kHz나 48kHz에 비해서는 절반밖에 안 되는 sampling rate이었기 때문에 high sampling rate TTS에 대한 수요가 꾸준히 있었습니다. 저희가 개발한 TTS에 대해서 더 높은 퀄리티로 서비스를 제공하기 위해서 <em>Neural Audio Upsampling</em>에 대한 연구를 진행하게 되었습니다. 48kHz라는 높은 sampling rate를 목표로 하는 연구를 진행하다 보니 자연스럽게 최근에 핫한 생성모델인 <em>diffusion model</em>을 적용하게 되었고 <strong>최초로 48kHz를 target으로 upsampling 하는데 성공</strong>하였습니다. 이렇게 진행한 연구로 <a href="https://github.com/Seungwoo0326" target="_blank" rel="noopener noreferrer">승우</a>님과 같이 쓴 paper가 <strong>세계 최고의 음성신호처리학회인 <a href="https://www.interspeech2021.org" target="_blank" rel="noopener noreferrer">INTERSPEECH 2021</a>에 Accept</strong>🎉 되어 소개해 드리고자 합니다!</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="audio-upsampling">Audio Upsampling<a class="hash-link" href="#audio-upsampling" title="Direct link to heading">​</a></h3><p>Image domain에서의 super resolution에 해당하는 분야가 <em>Audio Upsampling</em>입니다. Image super resolution에서 따와서 <em>audio super resolution</em> 혹은 주파수 범위 (bandwidth)를 넓힌다는 의미로 <em>bandwidth extension</em>이라고 부르기도 합니다. Audio의 경우 1초에 몇 번 sampling을 하는지를 나타내는 <em>Sampling Rate</em>로 temporal resolution을 표현합니다. 이 값이 중요한 이유는 sampling rate가 정해지면 discrete-time (or digital) audio data가 가질 수 있는 maximum frequency (Nyquist frequency)가 정해지기 때문인데요. Audio domain에 익숙하지 않으신 경우 아래 그림을 보시면 이해가 빠르실 것 같습니다.</p><ul><li>continuous-time 데이터 <img loading="lazy" alt="어떤 audio data" src="/assets/images/sine-9ec2c96c851cc3e7dcc1e7a578ef524d.png" width="1660" height="314"></li><li>1000Hz로 샘플링 했을 때  <img loading="lazy" alt="1000Hz로 샘플링 했을때" src="/assets/images/sr1000hz-58acc22915ef3d61f1c1311f8c41da6e.png" width="1502" height="337"></li><li>100Hz로 샘플링 했을 때 <img loading="lazy" alt="100Hz 로 샘플링 했을때" src="/assets/images/sr100hz-9ba2eaaa9681cfa08ca797e6677f8e72.png" width="1720" height="289"></li></ul><p>공기의 진동에 해당하는 소리를 audio data로 컴퓨터에서 다루기 위해서는 마이크에서 수음된 신호를 discretize하는 과정이 필요한데, 시간에 대해 discretize 하는 것을 sampling이라고 합니다. 원본 신호보다 sampling rate가 상당히 높을 경우 두 번째 그림처럼 신호에 대한 정보를 잘 담을 수 있습니다. 하지만 이 과정에서 신호의 주파수가 <em>N</em>일 때 sampling rate가 2<!-- -->*<em>N</em> 보다 작다면 신호에 대한 어떤 정보도 얻을 수 없는 노이즈가 되게 됩니다. 위의 사진에서 세 번째 그림이 그 케이스죠. 2<!-- -->*<em>N</em>인 경우 운이 좋다면 주파수 <em>N</em>으로 진동하는 신호가 있다 정도는 알아낼 수 있게 됩니다. </p><p>어떤 신호를 딥러닝 없이 linear, nearest interpolation 등의 방법으로 upsampling하게되면 위의 이유 때문에 원본의 sampling rate의 절반 이상에 해당하는 주파수는 포함하지 않게 됩니다 <del>정확히는 spectral alias가 생기기 때문에 post filtering을 하기 때문이지만 신호처리 시간이 아니니 넘어가죠</del>. 그래서 sampling rate 자체는 높아지지만 STFT (Short-Time Fourier Transform)을 했을 때 윗부분이 비어 보이게 됩니다. 이런 문제를 해결하기 위해서 딥러닝을 적용한 연구들이 있습니다. 크게 저희 연구와 비교한 케이스 두 개만 소개하고 넘어가겠습니다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="audio-super-resolution-using-neural-networkssup1sup">Audio Super Resolution Using Neural Networks<a href="#r1"><sup>[1]</sup></a><a class="hash-link" href="#audio-super-resolution-using-neural-networkssup1sup" title="Direct link to heading">​</a></h4><img class="figCenter_mKJ5" src="/assets/images/keenet-baf3f0ed9e6e3dec155b5823f711a6b2.png" alt="keenet"><p>Audio upsampling task에서 딥러닝을 적용한 첫 논문입니다. U-Net과 같은 구조를 적용하였고 <em>N</em> Hz 신호를 input으로 하고 <em>N<!-- -->*<!-- -->r</em> Hz 신호를 output으로 하는 모델입니다. Loss로 L<sub>2</sub>-norm을 사용하였습니다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="bandwidth-extension-on-raw-audio-via-generative-adversarial-networkssup2sup">Bandwidth Extension on Raw Audio via Generative Adversarial Networks<a href="#r2"><sup>[2]</sup></a><a class="hash-link" href="#bandwidth-extension-on-raw-audio-via-generative-adversarial-networkssup2sup" title="Direct link to heading">​</a></h4><img class="figCenter_mKJ5" src="/assets/images/mugan-3aeb03015f7ec5cc1219d1aa99e1c1c0.png" alt="mugan"><p>GAN을 사용해서 upsampling을 시도한 논문입니다. 구조 자체는 위의 논문과 비슷하고 loss로 L<sub>2</sub>-norm, discriminator loss, feature loss를 사용하였습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="diffusion-probabilistic-models">Diffusion Probabilistic Models<a class="hash-link" href="#diffusion-probabilistic-models" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_mojV" id="what-is-diffusion-model">What is Diffusion Model?<a class="hash-link" href="#what-is-diffusion-model" title="Direct link to heading">​</a></h4><p>최근 <em>Denoising Diffusion Probabilistic Model</em><a href="#r3"><sup>[3]</sup></a>을 필두로 <em>diffusion probabilistic model</em> (줄여서 <em>diffusion model</em>)이 핫한 생성모델로 떠오르고 있습니다. 더 넓은 범위의 <em>score-based model</em>이라는 것도 있으나 이 친구는 아마 다른 포스트로 소개할 것 같습니다. Diffusion model은 GAN이나 VAE와 다르게 output과 latent variable의 사이즈가 같습니다. latent varable은 각 step 마다 원본에 일정한 Gaussian noise가 더해진 것으로 정의됩니다. 이를 0부터 T까지의 step을 가지고 <em>forward/reverse</em> 두 개의 path를 가지는 Markov chain으로 생각합니다. Forward path의 경우 위에서 설명한대로 그 전 step에  Gaussian noise를 더하는 것으로 정의되고 reverse path 의 경우 forward path에서 더해진 Gaussian noise를 예측해서 빼는 것으로 정의됩니다. 결과적으로 Gaussian distribution으로부터 sampling한 latent variable을 여러 번 iteration을 하는 Markov Chain Monte-Carlo Sampling을 통해 noise를 제거해가면서 우리가 원하는 output으로 sampling하는 모델입니다. </p><img class="figCenter_mKJ5" src="/assets/images/mc-5f6f8ce73ca2b806164c34b56a69fa7d.png" alt="원래_논문에_넣으려고_했던_이미지"><p><em>원래 논문에 넣고 싶었는데 4p라 분량이 모자라 못 넣은 Markov Chain 이미지. forward path(점선), reverse path(직선)</em></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="training-and-sampling">Training and Sampling<a class="hash-link" href="#training-and-sampling" title="Direct link to heading">​</a></h4><p>원본 데이터에 임의로 노이즈를 더하고 더해진 노이즈를 모델이 예측하는 방식으로 학습을 진행하고 sampling시에는 Gaussian noise부터 시작하여 우리가 원하는 data distribution으로 가는 방향에 해당하는 noise (score)를 예측하여 빼주는 것을 반복하게 됩니다. Diffusion model은 sampling이 여러 번 반복되어야 하기 때문에 오래 걸린다는 단점이 있지만 매우 높은 퀄리티의 sample을 생성할 수 있습니다.</p><img class="figCenter_mKJ5" src="/assets/images/ddpm-85c710f98f04d34f5743f5e6d2ae5e6f.png" alt="ddpm"><p><em>DDPM의 training and sampling algoritihm</em></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="conditional-diffusion-models-as-a-neural-vocoder">Conditional Diffusion Models as a Neural Vocoder<a class="hash-link" href="#conditional-diffusion-models-as-a-neural-vocoder" title="Direct link to heading">​</a></h4><table><thead><tr><th>DiffWave Architecture</th><th>WaveGrad Architecture</th></tr></thead><tbody><tr><td><img loading="lazy" alt="DiffWave" src="/assets/images/diffwave-6da070936ba7db2f9ed945027935b669.png" width="1292" height="922"></td><td><img loading="lazy" alt="wavegrad" src="/assets/images/wavegrad-f28ddfc7bbfa30ea46fffdcd505021c0.png" width="1170" height="1190"></td></tr></tbody></table><p>Diffusion model이 audio domain에 가장 먼저 적용된 분야는 neural vocoder인데요, neural vocoder는 Mel<!-- -->-<!-- -->spectrogram을 input으로 받아 raw audio를 생성하는 모델입니다. <em>ICLR 2020</em>에 동시에 <em>DiffWave</em><a href="#r4"><sup>[4]</sup></a>와 <em>WaveGrad</em><a href="#r5"><sup>[5]</sup></a>라는 diffusion-based neural vocoder 논문이 나왔습니다. Mel-spectrogram을 local condition으로 주고  마찬가지로 noise로부터 iterative하게 sampling 합니다. Mel-spectrogram과 output인 raw waveform이 linear하게 align 되어 있다는 점은 audio upsampling과 유사하여 적극적으로 audio upsampling 연구에 차용하였습니다. </p><p>사실 이 연구는 위의 논문들을 읽고 너무 감명 받아<del>간지나서</del> 시작하게 되었습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="nu-wave">NU-Wave<a class="hash-link" href="#nu-wave" title="Direct link to heading">​</a></h3><p>처음엔 목표를 <strong>48kHz를 타겟으로 하는 neural audio upsampling을 하는 것</strong>으로 잡았습니다. 그리고 구현이 상대적으로 간단한 위에 소개했었던 연구들을 구현하여 진행하였습니다. 그런데 상상 이상으로 결과가 좋지 않았습니다. Low frequency 부분은 크게 건드리지 않았지만 high frequency 부분들은 너무 과하거나 거의 없거나 했습니다. 그러던 중 위에서 설명한 <em>diffusion model</em>이 vocoder task에서 좋은 결과를 내었고 image domain에서도 1024x1024 해상도의 이미지를 sampling한 결과가 있어 바로 적용해 보기로 하였습니다. </p><img class="figCenter_mKJ5" src="/assets/images/nuwave-b0ed027091ad1432d7bb68564a7816b8.png" alt="nuwave"><p><em>NU-Wave의 architecture와 algorithm</em></p><p>Neural vocoder 연구를 audio upsampling에 적용하기 위해 DiffWave와 WaveGrad를 많이 참고했습니다. 구조는 dilation convolution을 사용한 DiffWave의 구조가 U-Net like한 WaveGrad 보다 사이즈에 더 일반적일 것으로 생각하여 DIffWave의 구조와 최대한 유사하게 만들었고, WaveGrad의 경우 training 과정에서 continuous noise level training이라는 것을 적용하여 sampling시에 더 적은 수의 iteration으로 sampling을 하는 방법을 참고하였습니다. </p><p>Neural vocoder에서 사용된 값들을 upsampling에 사용하려고 하다 보니 몇가지 문제점이 있었습니다.</p><ol><li>Raw waveform을 condition으로 넣어줬을 때 receptive field가 너무 작아 condition이 별로 영향을 주지 못했다.</li><li>생각보다 sample이 많이 noisy 하다.</li><li>너무 high sampling rate를 target으로 하다 보니 computing power가 많이 필요했다.</li></ol><p>1번의 경우 여러 가지를 시도하다 condition signal에도 Bi-DilConv를 적용했더니 갑자기 엄청나게 잘되기 시작하였습니다. Condition signal의 receptive field를 Bi-DilConv로 넓혀준 것이 효과가 있었다는 게 저희가 분석한 결과입니다.</p><p>2번의 경우 WaveGrad에서 제시했던 noise schedule, linear scale 등의 수치를 상당히 조절하고 학습을 오래 했더니 해결되었습니다. 아직 diffusion model이 많이 연구되었던 것이 아니다 보니 이런 hyperparameter들을 직접 여러 번 테스트해보는 것밖에는 답이 없었습니다.</p><p>3번의 경우 좋은 GPU를 제공해주는 MINDs Lab이라서 A100 두 대를 사용해서 해결했습니다.</p><p>해결 방법을 여러 가지를 시도해서 해결했다고 쓰긴 했지만 이 기간이 한 달 좀 넘었던 것 같습니다. 1월에 본격적으로 시작하여 구현은 2주안에 끝났는데 hyperparameter만 한달 넘게 고쳐가면서 정말 여러 가지를 시도해봤습니다. 한 달 넘는 시도 끝에 좋은 sample을 뽑아내는 정도가 되어 3월에는 논문 작성에 박차를 가했던 것 같습니다.</p><img class="figCenter_mKJ5" src="/assets/images/sampling-503b47b072ff85dc85742fbd9951fd82.gif" alt="sampling"><p><em>NU-Wave가 8번 만에 sampling하는 과정</em></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="results">Results<a class="hash-link" href="#results" title="Direct link to heading">​</a></h3><img class="figCenter_mKJ5" src="/assets/images/result-58171d4126d4fe9718d297137c472746.png" alt="result"><p>앞에서 소개한 것들과의 비교를 위한 spectrogram입니다. 제일 왼쪽 원본에 비해서 가운데 세개의 경우 과하게 기둥이 섰거나, Nyquist frequency 기준으로 대칭이 되거나 하는 현상을 보입니다. 하지만 NU-Wave의 경우 자연스럽게 high frequency 부분을 생성하는 것을 볼 수 있습니다. </p><table><thead><tr><th>SNR, LSD</th><th>ABX accuracy</th></tr></thead><tbody><tr><td><img loading="lazy" src="/assets/images/objective-2ef1c4d260729597247570d554dac131.png" width="966" height="592"></td><td><img loading="lazy" src="/assets/images/subjective-fb53e1a9c71e1a880977fb76fb92320a.png" width="962" height="338"></td></tr></tbody></table><p>Audio upsampling task에서는 크게 3가지의 metric을 사용합니다. 원본과 downsampling한 신호를 다시 upsampling한 후 수치적으로 비교하는 <em>SNR (signal-to-noise ratio)</em>, <em>LSD (log-spectral distance)</em>와 사람이 다른 종류의 A, B와 둘 중 하나를 뽑은 X 세가지를 들어보고 어느것인지 구분할 수 있는 ABX test의 정확도인 <em>ABX accuracy</em>가 있습니다. <strong>NU-Wave의 경우 SNR, LSD, ABX acc에서 3.0M의 parameter로 다른 모델들보다 높은 성능을 보였습니다.</strong> </p><h3 class="anchor anchorWithStickyNavbar_mojV" id="discussion">Discussion<a class="hash-link" href="#discussion" title="Direct link to heading">​</a></h3><p>Downsampling 방식을 한정지어 아직 실제로 upsampling을 할 때는 문제가 생긴다는 점, diffusion model의 특성인지 high noise가 살짝 남는다는 점 등 개선해야할 부분이 많고 원래의 목표인 TTS에 적용하는 부분도 숙제로 남아있는것 같습니다. 자세한 내용은 <a href="https://arxiv.org/abs/2104.02321" target="_blank" rel="noopener noreferrer">논문</a>을 참고해주시고 <a href="https://github.com/mindslab-ai/nuwave" target="_blank" rel="noopener noreferrer">코드</a>에 많은 스타⭐부탁드립니다. </p><p>9월에 INTERSPEECH 학회에서 봬요! ​</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h3><a name="r1"></a><ol><li>V. Kuleshov, S. Z. Enam, and S. Ermon, “Audio super resolution using neural networks,” in <em>Workshop of International Conference on Learning Representations</em>, 2017. <a href="https://arxiv.org/abs/1708.00853" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><a name="r2"></a><ol start="2"><li>S. Kim and V. Sathe, “Bandwidth extension on raw audio via generative adversarial networks,” <em>arXiv preprint arXiv:1903.09027</em>, 2019. <a href="https://arxiv.org/abs/1903.09027" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><a name="r3"></a><ol start="3"><li>J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” in <em>Advances in Neural Information Processing Systems</em>, 2020, pp. 6840–6851. <a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><a name="r4"></a><ol start="4"><li>Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, “Diffwave: A versatile diffusion model for audio synthesis,” in <em>International Conference on Learning Representations</em>, 2021. <a href="https://arxiv.org/abs/2009.09761" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><a name="r5"></a><ol start="5"><li>N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan, “Wavegrad: Estimating gradients for waveform generation,” in <em>International Conference on Learning Representations</em>, 2021. <a href="https://arxiv.org/abs/2009.00713" target="_blank" rel="noopener noreferrer">[arxiv]</a></li></ol><h3 class="anchor anchorWithStickyNavbar_mojV" id="tldr">TL;DR<a class="hash-link" href="#tldr" title="Direct link to heading">​</a></h3><ol><li><strong>최초로 16kHz/24kHz에서 48kHz로 upsampling 성공</strong></li><li><strong>최초로 diffusion model을 audio upsampling에 적용</strong></li><li><strong>적은 parameter number(3.0M)으로 다른 모델들을 능가</strong></li></ol></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_h6_j"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/publication">publication</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/paper-review">paper-review</a></li></ul></div></footer></article><div></div><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/acon"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Activate or Not: Learning Customized Activation</div></a></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></main><div class="col col--2"><div class="tableOfContents_cNA8 thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#a-diffusion-probabilistic-model-for-neural-audio-upsampling" class="table-of-contents__link toc-highlight">A Diffusion Probabilistic Model for Neural Audio Upsampling</a><ul><li><a href="#audio-upsampling" class="table-of-contents__link toc-highlight">Audio Upsampling</a></li><li><a href="#diffusion-probabilistic-models" class="table-of-contents__link toc-highlight">Diffusion Probabilistic Models</a></li><li><a href="#nu-wave" class="table-of-contents__link toc-highlight">NU-Wave</a></li><li><a href="#results" class="table-of-contents__link toc-highlight">Results</a></li><li><a href="#discussion" class="table-of-contents__link toc-highlight">Discussion</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li><li><a href="#tldr" class="table-of-contents__link toc-highlight">TL;DR</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Contents</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/publications">Publications</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/maum-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/careers">Careers</a></li><li class="footer__item"><a class="footer__link-item" href="/blog/tags">Tags</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 maum.ai BRAIN Team. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.685fb885.js"></script>
<script src="/assets/js/main.4fd61293.js"></script>
</body>
</html>