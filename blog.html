<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="maum.ai BRAIN Team RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="maum.ai BRAIN Team Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-204903244-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">Blog | maum.ai BRAIN Team</title><meta data-rh="true" property="og:title" content="Blog | maum.ai BRAIN Team"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" property="og:url" content="https://maum-ai.github.io//blog"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://maum-ai.github.io//blog"><link data-rh="true" rel="alternate" href="https://maum-ai.github.io//blog" hreflang="en"><link data-rh="true" rel="alternate" href="https://maum-ai.github.io//blog" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.def04243.css">
<link rel="preload" href="/assets/js/runtime~main.8a7caefc.js" as="script">
<link rel="preload" href="/assets/js/main.e6055739.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/mindslab_brain.svg" alt="maum.ai BRAIN Team" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/mindslab_brain.svg" alt="maum.ai BRAIN Team" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title"></b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/open-source">Open-Source</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/careers">Careers</a><a href="https://github.com/maum-ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">All posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/birp">BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/cbits">CBITS: CryptoBERT Incorporated Trading System</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/nu-wave2">NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/sane-tts">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/assem-vc">Assem-VC and Assem-Singer:</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/ces-2022-review">CES 2022에 참석한 마인즈랩, AI Human</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/vits">VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/acon">Activate or Not: Learning Customized Activation</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/nu-wave">NU-Wave(Interspeech):</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/birp">BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2023-06-04T00:00:00.000Z" itemprop="datePublished">June 4, 2023</time> · <!-- -->17 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/puzzlecollector" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/puzzlecollector.png" alt="Minsuk Kim"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/puzzlecollector" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Minsuk Kim</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (NLP)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div class="markdown" itemprop="articleBody"><p><a href="https://github.com/puzzlecollector/AI-Conferences/blob/main/SIGIR_SIRIP_2023_final.pdf" target="_blank" rel="noopener noreferrer">KDD Workshop on Machine Learning in Finance</a></p><blockquote><p>Minsuk Kim, Gyeongmin Kim, Byungchul Kim, Junyeong Yong, Jeongwoo Park. &quot;BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching&quot;</p><p>KDD Workshop</p></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="birp-bitcoin-information-retrieval-prediction-model-based-on-multimodal-pattern-matching">BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching<a class="hash-link" href="#birp-bitcoin-information-retrieval-prediction-model-based-on-multimodal-pattern-matching" title="Direct link to heading">​</a></h2><p>안녕하세요, 마인즈랩 BRAIN NLP팀에서 근무하고 있는 김민석입니다. 이번에 <a href="https://sites.google.com/view/kdd-mlf-2023" target="_blank" rel="noopener noreferrer">KDD workshop</a>에 제출한 작업물에 대해서 이 블로그 포스트를 통해 간략하게 설명해보려합니다.
이전의 제 블로그 포스트 <a href="https://mindslab-ai.github.io/blog/cbits" target="_blank" rel="noopener noreferrer">CBITS</a>처럼 논문의 모든 부분을 설명한다기보다 중요한 부분만 간략하게 설명할 예정이니 자세한 내용은 논문을 통해 확인해주시면 감사하겠습니다.
이번 포스트는 이전 CBITS에서 소개한 한글 크립토 언어모델과 트레이딩 봇 프레임워크를 활용해서 이전 블로그 포스트 내용을 아시면 이 포스트도 읽는데 도움이 될 것 같습니다. 추가적으로 CBITS를 최근에 거의 한달간 (대략 3월 18일 ~ 4월 18)
실전투자를 돌리면서 수익률을 기록해둔 페이지가 있는데 <a href="https://puzzlecollector.github.io/bot_performance/liverun.html" target="_blank" rel="noopener noreferrer">여기서</a> 확인해보실 수 있습니다. 실투를 진행하면서 어떻게 봇을 고도화 시킬 수 있는지에 대한 많은 아이디어들을 얻었습니다.
그리고 한글 크립토 언어모델들은 제 huggingface organization
에 전부 공개해놔서 <a href="https://huggingface.co/axiomlabs" target="_blank" rel="noopener noreferrer">여기서</a> 다운받아서 사용해보실 수 있습니다. 모델들은 추가적인 사전학습이나 파인튜닝이 진행되면서 계속 업데이트를 할 예정입니다.</p><p>이번 논문은 short paper, 그리고 industry쪽에 초점을 맞추는 방향으로 작성을해서 내용이 짧고 가볍습니다. 주식이나 코인쪽에서 다양한 패턴매칭 툴이 존재합니다. 대부분 패턴매칭 툴은 현재 차트 구간이 주어지면 과거 유사한 차트 구간을
포착해서 트레이더에게 보여주는 방식으로 동작합니다. 그럼 트레이더는 해당 과거 유사한 구간을 보고 그 당시에 가격이 어떤식으로 흘러갔는지 참고해서 현재 투자에 있어서 가장 좋은 행동은 무엇일지 결정합니다. 코인쪽에서 이러한
패턴매칭 툴의 예시는 <a href="http://trendspider.com" target="_blank" rel="noopener noreferrer">trendspider</a> 아니면 <a href="https://miningcalc.kr/chart/btc" target="_blank" rel="noopener noreferrer">miningcalc.kr</a> 정도가 먼저 생각납니다. 하지만 이러한 대중적으로 공개된 패턴 매칭 지표들은 크게 두가지
한계점이 있다고 생각했습니다. (1) 일차원적으로 차트 데이터만 고려해서 패턴 매칭을 진행합니다. (2) 유사 차트 구간을 추천하고 이후의 행동은 모두 트레이더에게 맡깁니다. 이 두가지 문제점을 address하기 위해서, 저희는 차트 데이터와
텍스트 데이터 (코인니스 뉴스 속보)를 둘 다 활용해서 패턴 매칭을 하는 방법을 제시했고 이러한 유사 차트 구간 정보를 어떻게 모델링에 활용하면 좋을지에 대한 아이디어와 실제로 구현했을때 이러한 방법이 투자에 있어서 도움이 될지를 보여줬습니다.
논문 제목에서 볼 수 있듯이 비트코인 가격에 대한 패턴 매칭을 진행했지만, 여기에서 제시하는 방법론은 적절한 데이터만 구할 수 있다면 다른 종목으로도 충분히 확장이 가능해보입니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="contributions">Contributions<a class="hash-link" href="#contributions" title="Direct link to heading">​</a></h2><ul><li>차트 데이터와 코인니스 속보 텍스트 데이터를 활용한 멀티모덜 비트코인 가격의 패턴매칭 방법을 제시합니다.</li><li>멀티모덜 패턴매칭 방법으로 얻은 정보를 비트코인 방향성 예측 모델링에 활용하는 방안을 제시하고, 이 방법이 패턴매칭 정보를 활용하지 않을때보다 더 유용하다는걸 실험적으로 보여줍니다.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="비트코인-방향성-분류-프레임워크에-멀티모덜-패턴매칭-정보-사용하기">비트코인 방향성 분류 프레임워크에 멀티모덜 패턴매칭 정보 사용하기<a class="hash-link" href="#비트코인-방향성-분류-프레임워크에-멀티모덜-패턴매칭-정보-사용하기" title="Direct link to heading">​</a></h2><p>전체적인 비트코인 방향성 분류 프레임워크는 이전 <a href="https://mindslab-ai.github.io/blog/cbits" target="_blank" rel="noopener noreferrer">CBITS</a>와 매우 유사합니다. Ternary classification으로 4시간 단위로 접근하되, 이전 CBITS에서는 뉴스 데이터의 감성 점수를 차트 데이터와 활용했다면,
이번 BIRP는 특정 방식으로 랭크된 과거 차트 구간들의 voting 정보를 피처로 활용합니다 (정확히는 normalize된 voting 정보를 활용합니다). 아래 Figure 1에 전체적인 프레임워크를 묘사했습니다.</p><table><thead><tr><th align="center">BIRP Architecture</th></tr></thead><tbody><tr><td align="center"><img class="figCenter_mKJ5" src="/assets/images/birp_diagram-c4eb63899f816f8b72b433d52a35ce23.png" alt="BIRP Architecture"></td></tr></tbody></table><p>저희는 네가지 랭킹 방식을 제시했는데 다음과 같습니다:</p><ul><li>random<ul><li>현재 차트 구간이 주어지면 과거 차트 구간 중 랜덤하게 30개를 샘플해서 가져옵니다.</li></ul></li><li>euclidean<ul><li>현재 차트 구간이 주어지면 과거 차트 구간 중 현재 차트 구간과의 L2-distance가 가장 가까운 구간 top 30개를 랭크해서 가져옵니다.</li></ul></li><li>TS2Vec<ul><li>비지도 방식으로 시계열 데이터의 특성들을 학습해서 다양한 downstream task에 대해 효율적으로 임베딩을 뽑는 방식인 TS2Vec 모델을 사용해서 시계열 임베딩을 계산합니다. 현재 차트 구간의 임베딩을 계산하고, 현재 구간의 임베딩과 코사인 거리가 가장 가까운 과거 차트 구간들의 임베딩들 top 30개를 랭크해서 가져옵니다.</li></ul></li><li>Multimodal<ul><li>CryptoRoBERTa를 이용해서 해당 차트 구간동안 나온 모든 뉴스 데이터의 평균 임베딩을 계산하고, 해당 차트 구간의 시계열 데이터의 임베딩을 TS2Vec으로 계산합니다. 이후에 평균 뉴스 임베딩과 시게열 데이터 임베딩의 합해서 멀티모덜 임베딩을 구합니다. 그리고 이러한 멀티모덜 임베딩들의 코사인 유사도가 가장 가까운 순으로 top 30개를 랭크해서 가져옵니다.</li><li>뉴스 임베딩과 차트 임베딩을 더할때 둘의 차원을 맞추기 위해서 뉴스 임베딩의 차원을 UMAP으로 줄여주는 과정을 거쳤습니다.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="실험-데이터">실험 데이터<a class="hash-link" href="#실험-데이터" title="Direct link to heading">​</a></h2><p>데이터는 이전 <a href="https://mindslab-ai.github.io/blog/cbits" target="_blank" rel="noopener noreferrer">CBITS</a>와 비슷하게 바이낸스에서 긁어온 비트코인 차트 데이터와 코인니스에서 얻은 속보형 뉴스 데이터 입니다.
전체 차트와 뉴스 기간은 2017-08-23 16:00:00부터 2023-01-15 20:00:00 까지였고 (11,812개의 데이터 포인트), 차트 데이터는 4시간봉으로 긁어왔습니다. 과거 유사한 구간을 랭킹하는 작업이 필요하기 때문에 풍부한 양의 candidate data가
필요해서 첫 80%의 데이터를 candidate로 나머지 20%의 데이터를 train/validation/test set으로 나눠서 실험을 진행했습니다. train/validation/test set의 기간은 2021-12-18 04:00 부터 2023-01-15 20:00:00까지였고
총 2,363개의 데이터 포인트로 구성이 되어 있었습니다. train/validation/test의 비율은 8:1:1로 진행되었고 각 2,363개의 데이터포인트는 candidate 데이터와 특정 랭킹 방법 (multimodal, euclidean, ts2vec, random)으로 비교되어 해당 데이터포인트와 가장 유사한 30개 데이터포인트들을 미리 계산해뒀습니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="다른-패턴매칭-방법들과-멀티모덜-패턴매칭-방법의-성능-비교와-backtest-결과">다른 패턴매칭 방법들과 멀티모덜 패턴매칭 방법의 성능 비교와 Backtest 결과<a class="hash-link" href="#다른-패턴매칭-방법들과-멀티모덜-패턴매칭-방법의-성능-비교와-backtest-결과" title="Direct link to heading">​</a></h2><p>CBITS와 비슷한 프레임워크와 모델링 방식을 사용했지만 사용하는 모델 자체는 XGBoost하나로만 제한시키고 다양한 랭킹 방법을 비교하는 방식으로 실험을 진행했습니다. 만약 시간적 여유가 더 있었다면, 다른 모델들도 활용해서 여러가지 조합을 실험해 볼 수 있었을 것 같습니다.</p><img class="figCenter_mKJ5 medium_PfCl" src="/assets/images/chart_vote_performance-a807d988fd9be135f72c9948ee40b263.png" alt="Table1"><p>voting 정보를 top 5, 10, 15, 20, 25, 30를 활용하는 방법으로 진행했는데 보다시피 multimodal 방식의 랭킹을 이용해서 voting feature를 사용하는게 이러한 정보를 사용하지 않는것과 비교해서 가장 높은 성능 향상을 보였습니다.</p><p>그리고 top 5, 혹은 top 10을 활용하는게 대부분의 경우 결과가 가장 좋아보입니다. 추측이지만 너무 많은 과거 유사 구간들로부터 voting 정보를 계산하면 noise가 데이터에 너무 많이 추가되는 경향이 있는 것 같습니다.</p><p>정확도나 F1 수치 자체로만 본다면 아직 개선해야할점이 많지만 유사도 기반 랭킹으로 얻은 voting 정보를 활용하는게 유용하단걸 보여주기에는 충분해 보입니다.</p><table><thead><tr><th align="center">Backtest 결과</th></tr></thead><tbody><tr><td align="center"><img class="figCenter_mKJ5" src="/assets/images/backtest-4cb1dc8a7a24fa826e5ce874d104edb0.png" alt="backtest"></td></tr></tbody></table><p>multimodal top 5 방식의 랭킹으로 voting 정보를 계산해서 backtest를 진행했을때 test set에서 buy and hold를 이기는 모습을 보여주면서 이 방식을 고도화시키면 수익률이 뛰어난 모델을 만들 수 있을 것 같습니다 (이전 CBITS와는 다르게 논문 제출 당시 아쉽게도 실투 결과를 포함할 시간은 없었습니다).</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h2><p>차트 데이터뿐만 아니라 뉴스 데이터도 활용해서 멀티모덜한 방법으로 유사한 과거 차트 구간을 랭킹하는 방법을 제시했습니다. 차트, 뉴스외에도 더 많은 정보를 포함한 랭킹 방식도 미래에 개발할 수 있을 것 같습니다 (비트코인의 경우 BTC dominance, Dollar index, on chain data같은 정보도 포함해서 유사도 랭킹에 활용).
그리고 이렇게 랭크해서 얻은 과거 유사 차트 구간정보를 활용해서 비트코인 방향성 예측에 도움이 되는 피처로서 활용할 수 있다는것도 실험적으로 보여줬습니다.</p><p>트레이딩 봇 뿐만 아니라 이러한 임베딩 정보를 활용한 유사 코인 뉴스 검색기나 패턴 매칭 툴도 만들어서 빠른시일내에 서비스를 진행할 계획도 있습니다. 아래 예시는 유저가 쿼리 뉴스를 입력하면, 해당 쿼리 뉴스와 유사하다고 판단된 과거 뉴스를 랭킹해서 보여주고 해당 뉴스가 나온 시점의 비트코인 차트 움직임도 보여줘서 (30분봉 기준으로) 투자자가 참고할 수 있게 합니다.
이 논문 작업을 진행하면서 매우 다양한 비트코인 투자 지표 아이디어가 떠올라서 이러한 아이디어들들 구현해보고 서비스해보는 목표가 생겼습니다.</p><table><thead><tr><th align="center">임베딩을 활용한 랭킹 지표 예시</th></tr></thead><tbody><tr><td align="center"><img class="figCenter_mKJ5" src="/assets/images/ranking_indicator-557e5c09d7485865a892dd153740f5a8.png" alt="backtest"></td></tr></tbody></table><p>트레이딩 관련 논문을 하나 더 준비중인데, 완성이 되면 블로그에 또 업데이트하도록 하겠습니다. 글 읽어주셔서 감사합니다!</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h2><a name="r1"></a><p>Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (San Francisco, California, USA) (KDD ’16). ACM, New York, NY, USA, 785–794. <a href="https://doi.org/10.1145/2939672.2939785" target="_blank" rel="noopener noreferrer">https://doi.org/10.1145/2939672.2939785</a></p><a name="r2"></a><p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 1597–1607.</p><a name="r3"></a><p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 4171–4186. <a href="https://doi.org/10.18653/v1/N19-1423" target="_blank" rel="noopener noreferrer">https://doi.org/10.18653/v1/N19-1423</a></p><a name="r4"></a><p>Guozhu Dong and Vahid Taslimitehrani. 2016. Pattern-aided regression modeling and prediction model analysis. In 2016 IEEE 32nd International Conference on Data Engineering (ICDE). 1508–1509. <a href="https://doi.org/10.1109/ICDE.2016.7498398" target="_blank" rel="noopener noreferrer">https://doi.org/10.1109/ICDE.2016.7498398</a></p><a name="r5"></a><p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 6894–6910. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.552" target="_blank" rel="noopener noreferrer">https://doi.org/10.18653/v1/2021.emnlp-main.552</a></p><a name="r6"></a><p>Xueyuan Gong and Yain-Whar Si. 2013. Comparison of subsequence pattern matching methods for financial time series. In 2013 Ninth International Conference on Computational Intelligence and Security. IEEE, 154–158.</p><a name="r7"></a><p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION. In International Conference on Learning Representations. 1–21. <a href="https://openreview.net/forum?id=XPZIaotutsD" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=XPZIaotutsD</a></p><a name="r8"></a><p>Gyeongmin Kim, Minsuk Kim, Byungchul Kim, and Heuiseok Lim. 2023. CBITS: Crypto BERT Incorporated Trading System. IEEE Access 11 (2023), 6912–6921. <a href="https://doi.org/10.1109/ACCESS.2023.3236032" target="_blank" rel="noopener noreferrer">https://doi.org/10.1109/ACCESS.2023.3236032</a></p><a name="r9"></a><p>Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426 (2018).</p><a name="r10"></a><p>Satoshi Nakamoto. 2008. Bitcoin: A peer-to-peer electronic cash system. Decentralized Business Review (2008), 21260.</p><a name="r11"></a><p>Marc Velay and Fabrice Daniel. 2018. Stock chart pattern recognition with deep learning. arXiv preprint arXiv:1808.00418 (2018)</p><a name="r12"></a><p>Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. 2022. Ts2vec: Towards universal representation of time series. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 8980–8987.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/publication">publication</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/paper-review">paper-review</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/cbits">CBITS: CryptoBERT Incorporated Trading System</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2023-01-16T00:00:00.000Z" itemprop="datePublished">January 16, 2023</time> · <!-- -->19 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/puzzlecollector" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/puzzlecollector.png" alt="Minsuk Kim"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/puzzlecollector" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Minsuk Kim</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (NLP)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div class="markdown" itemprop="articleBody"><p><a href="https://ieeexplore.ieee.org/document/10014986" target="_blank" rel="noopener noreferrer">IEEE Early Access</a></p><blockquote><p>Gyeongmin Kim, Minsuk Kim, Byungchul Kim, Heuiseok Lim. &quot;CBITS: CryptoBERT Incorporated Trading System&quot;</p><p>IEEE Access</p></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="cbits-cryptobert-incorporated-trading-system">CBITS: CryptoBERT Incorporated Trading System<a class="hash-link" href="#cbits-cryptobert-incorporated-trading-system" title="Direct link to heading">​</a></h2><p>안녕하세요, 마인즈랩 BRAIN NLP팀에서 근무하고 있는 김민석입니다. 이번에 IEEE Access에 accept를 받은 작업물에 대해서 이 블로그 포스트를 통해 간략하게 설명해보려합니다.
논문의 모든 부분을 설명한다기보다 중요한 부분만 간략하게 설명할 예정이니 자세한 내용은 논문을 통해 확인해주시면 감사하겠습니다.</p><p>평소에도 자연어 데이터를 사용한 금융 상품 가격 예측에 관심이 많은 편이고 특히 뉴스, 속보, 트윗등에 매우 민감한 자산인 암호화폐, 그리고 그중에서도 대장격인 비트코인의 추세 예측에 관심이 많았습니다.
따라서 암호화폐 분야에 domain-specific 언어모델을 사전학습 시켜봐야겠다고 생각했고, 공개된 한국어 언어모델이 많이 없는 상황이니 아예 한국어 사전학습된 암호화폐 language model (LM)을 만들어보자는 취지에서 시작하게 된 프로젝트입니다 (한국어 언어모델에 고집한 이유는 사실 더 있고 아래 추가로 설명하겠습니다).
현재는 huggingface에 공개된 영문 <a href="https://huggingface.co/ElKulako/cryptobert" target="_blank" rel="noopener noreferrer">cryptobert</a> 모델이 존재하긴하지만, 저희가 작업했을 당시에는 암호화폐쪽의 domain specific한 언어모델이 존재하지 않았습니다. 저희가 진행한 프로젝트가 거의 최초의 암호화폐 domain-specific한 언어모델을 만드려는 시도가 아니었을까 생각합니다.
만들어진 암호화폐 언어모델들을 활용해서 비트코인 선물시장 (BTC USDT Perpetual)에서 거래를 하는 투자봇을 만들어봤고 실전투자에도 투입시켜서 수익을 달성하는걸 확인했습니다. 매우 간단한 시도라서 사실 만들어진 언어모델로 훨씬 고도화된 시스템도 만들 수 있고, 단순 투자봇뿐만 아니라 암호화폐 관련 텍스트 유사도 측정등의 다양한 테스크에도
사용될 수 있다고 생각합니다. 이번 작업물이 accept를 받음으로써 crypto LM을 활용한 후속 연구를 추가로 더 하고싶은 motivation도 생겼습니다. 저희 한국어 <a href="https://huggingface.co/totoro4007" target="_blank" rel="noopener noreferrer">crypto 언어모델들의 체크포인트는 huggingface에 공개되어 있습니다</a>.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="contributions">Contributions<a class="hash-link" href="#contributions" title="Direct link to heading">​</a></h3><ul><li>코인니스 뉴스로 구성된 비트코인 가격에 대해서 해당 뉴스가 호재성인지, 악재성인지, 중립인지 레이블된 파인튜닝 데이터셋을 제작했습니다.</li><li>암호화폐 사전학습용 corpus를 구축하고 세가지 언어모델 (BERT, RoBERTa, DeBERTa)를 사전학습 시키고, 위에서 언급한 파인튜닝 데이터셋으로 비트코인 감성분류 테스크에 대해서 학습시켰습니다.</li><li>비트코인 선물시장에서 동작하는 간단한 분류기반 트레이딩 프레임워크를 제시했습니다.</li><li>해당 트레이딩 프레임워크에 crypto 언어모델을 사용하여 전처리된 정보를 추가로 사용하면 성능이 향상된다는걸 증명했습니다.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="한국어-crypto-언어모델을-만든-이유">한국어 crypto 언어모델을 만든 이유<a class="hash-link" href="#한국어-crypto-언어모델을-만든-이유" title="Direct link to heading">​</a></h3><p>논문에서는 한국어 모델을 만든 여러가지 이유에 대해서 설명하지만 가장 결정적인 두가지 이유는:</p><ul><li>코인니스라는 뉴스 소스에서 나오는 정보를 실시간으로 해석하는 언어모델을 만들고 싶었고 이 소스는 한국어로된 소스입니다</li><li>이렇게 만들어진 한국어 암호화폐 언어모델로 다양한 암호화폐 관련된 지표, 툴 혹은 투자봇을 만들어서 나중에 한국 시장을 상대로 서비스를 해보고 싶었습니다.</li></ul><p>이 코인니스라는 뉴스 소스에 대해서 설명을 덧붙이자면 24시간, 365일동안 속보형식으로 구독자에게 텔레그램을 통해서 전달해주고 내용은 다양한 뉴스 소스들, 트위터, 그리고 온체인 모니터링 사이트 (e.g. whale alert) 같은 소스들을 계속 모니터링하여 정보를 정리해서 전달해줍니다.
하루에 평균 100-150개 정도의 속보가 올라옵니다. 텍스트 형식의 속보뿐만 아니라 다양한 지표 (RSI, fear-greed index 등등)과 같은 정보들도 정리해서 올라옵니다. 거기에 구독자들이 직접 참여해서 해당 뉴스가 bullish한 내용을 담고 있는지 bearish한 내용을 담고 있는지 투표를 하는 기능도 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="데이터-제작과정">데이터 제작과정<a class="hash-link" href="#데이터-제작과정" title="Direct link to heading">​</a></h3><p>Pre-training용 corpus의 경우, general text + crypto domain specific text로 진행되었고, crypto domain specific text는 다양한 한글 뉴스 소스 (파인튜닝에 사용된 코인니스 뉴스들은 제외), 해시넷같은 암호화폐 위키 관련 글들, 한국어로된 암호화폐 관련 논문, 암호화폐 관련 블로그글, 거래소에 실려있는 암호화폐 관련 설명글, 커뮤니티 사이등
다양한 소스에서 웹 크롤링을 실시해서 대략 1GB의 텍스트로 구성이 되었습니다. 시간적 여유가 많지 않아서 매우 큰 사전학습 corpus를 구축하지는 못했지만 그래도 아래 보이는 결과표처럼 암호화폐 domain에 사전학습 되지 않은 모델들보다 감성분류 테스크에서 성능이 더 좋은 모습을 보였습니다.</p><p>Fine-tuning 데이터의 경우, 코인니스에서 2018-01-19 부터 2022-04-16기간동안의 뉴스를 긁어와서 해당 뉴스들이 비트코인 가격에 대해서 호재, 악재, 중립인지 레이블링을 진행했습니다. 레이블링은 저와 다른 한명의 전업투자자가 진행을 했고, 시중에 나와있는 FinBERT모델 (그당시에는 영문 cryptobert도 존재하지 않았습니다)이
계산한 해당 뉴스의 감성점수, 코인니스의 bull/bear vote, 그리고 해당 뉴스가 나온 시점에 비트코인 가격이 어떻게 변동했는지등의 정보도 고려해서 레이블링이 진행되었습니다. 레이블링을 할때 비트코인 가격에 어떻게 영향을 끼칠지 위주로 레이블링을 해야했기 때문에 기준표를 만들어서 레이블러들이 기준표를 참고해서 레이블링을 진행하기로 합의를 봤었습니다.
총 18가지 기준이 있었고 해당 기준은 논문의 annotation guidelines에서 찾아보실 수 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="비트코인-감성-분류-태스크-성능">비트코인 감성 분류 태스크 성능<a class="hash-link" href="#비트코인-감성-분류-태스크-성능" title="Direct link to heading">​</a></h3><img class="figCenter_mKJ5 medium_PfCl" src="/assets/images/finetuning_performance-269cd818c1989cb8a84f11f110894cd8.png" alt="Table0"><p>총 3개의 언어모델을 사전학습 시키고 fine-tuning 시켰습니다. CryptoBERT, CryptoDeBERTa는 마인즈랩 BRAIN-NLP에 근무하시는 용준영 선임님이 사전학습을 진행하셨고, CryptoRoBERTa는 고려대학교 박사과정이신 김경민님이 진행했습니다.
Fine-tuning후에 downstream task에서의 성능을 저희 crypto LM들과 공개된 다국어 모델 (mBERT, XLM-RoBERTa)와 비교했을때 저희 crypto LM들의 성능 (F1 Score)가 더 우수한걸 확인했습니다. Crypto LM중에서 CryptoRoBERTa가 fine-tuning후에 가장 성능이 좋아서
이후에 언급된 실험들에는 CryptoRoBERTa가 사용되었습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="간단한-분류기반-트레이딩-프레임워크">간단한 분류기반 트레이딩 프레임워크<a class="hash-link" href="#간단한-분류기반-트레이딩-프레임워크" title="Direct link to heading">​</a></h3><p>만들어진 crypto LM을 가지고 매우 다양한 태스크를 수행할 수 있지만, 저희는 우선 이 모델들을 활용한 간단한 자동 투자봇을 제안했습니다.
투자봇은 naive하게 4시간마다 거래를 하게끔 만들었고, 수행할 수 있는 액션이 3개중 한개입니다: Long, Short, Hold.
현재시간부터 이후 4시간동안 가격이 0.75% 이상 오를 것 같으면 레이블 0 (Long), 0.75% 이상 내릴 것 같으면 레이블 1 (Short), 0.75% 미만으로 변동할 것 같으면 레이블 2 (Hold)로 설정하고 모델 학습을 진행했습니다.</p><table><thead><tr><th align="center">Labeling Rule</th></tr></thead><tbody><tr><td align="center"><img class="figCenter_mKJ5" src="/assets/images/labeling_rule-4d5b315eb5cab0120f3759c24847275f.png" alt="Labeling Rule"></td></tr></tbody></table><p>이렇게 학습해서 모델의 예측 성능이 좋다면, 비트코인의 상승장, 하락장, 횡보장 상관없이 모델이 대응할 수 있고 수익을 낼 수 있다고 생각했습니다. 이런 프레임워크에서 crypto LM을 통해서 계산된 감성점수를 추가로 활용하면 투자봇의 예측성능이 더 좋아질까?가 저희가 실험하고 싶었던 부분이었습니다.</p><table><thead><tr><th align="center">CBITS Architecture</th></tr></thead><tbody><tr><td align="center"><img class="figCenter_mKJ5" src="/assets/images/CBITS_Diagram-a3a2df60cd7802e1d9a02c6405fc4546.png" alt="CBITS Architecture"></td></tr></tbody></table><p>감성점수를 모델에 사용하는 방법이 여러가지가 있는데 해당 논문에서는 매우 간단한 접근법을 실험해봤습니다. 4시간동안 나온 뉴스들의 평균 감성점수를 계산하고, 이 감성점수와 차트 입력 피처들을 모델에 입력으로 넣어서 학습해주는 방법을 사용했습니다.
차트 입력 피처들은 단순히 open, high, low, close, volume뿐만 아니라 다양한 지표들 (주로 volatility와 관련된)을 사용했고, 학습에 사용한 모델들은 LSTM, XGBoost, TabNet 이렇게 3가지였습니다.
입력 데이터를 tabular 형식으로 전처리해서 접근했기 때문에 tabular 데이터에 강한 XGBoost 그리고 XGBoost보다 다양한 benchmark task에서 성능이 더 좋았다는 TabNet을 사용해서 실험하기로 했었고 결과는 TabNet이 가장 우수했습니다.</p><img class="figCenter_mKJ5 medium_PfCl" src="/assets/images/chart_news_model_performance-dafa0c441d4a2e3e5158e92b45a3f0e3.png" alt="Table1"><p>뉴스 감성점수를 추가하니 모든 모델의 accuracy와 F1 score이 더 좋아진걸 볼 수 있습니다. 추가적으로 다른 재밌는 실험도 진행했는데 가끔 매우 긴 코인니스 뉴스/속보의 경우 BERT계열 모델의 512 토큰 제한때문에 truncation이 이루어집니다.
뉴스 제목과 가장 유사한 본문 문장 top k (k = 5, 10)개를 이용해서 감성점수를 뽑았을때 몇몇 모델의 경우 추가 성능 향상이 있다는걸 확인했습니다.</p><img class="figCenter_mKJ5 medium_PfCl" src="/assets/images/chart_news_model_performance_topk-d6726e8084c8a7924c711b6e68f11bf2.png" alt="Table2"><h3 class="anchor anchorWithStickyNavbar_mojV" id="backtest-성능비교">Backtest 성능비교<a class="hash-link" href="#backtest-성능비교" title="Direct link to heading">​</a></h3><p>Test set에 대해서 단순 accuracy와 F1 점수보다는 실제로 이 트레이딩 프레임워크가 얼마만큼의 수익률을 내는지가 더 유용한 지표일거라고 생각해서 백테스트도 진행했습니다.
저희 모델의 경우 가장 성능이 좋았던 TabNet 모델들과 다른 트레이딩 기법들 (랜덤하게 투자하는 Monkey Trader, 단순 Buy and Hold, OLMAR)와 비교를 했고, 저희 모델의 경우 take profit 0.75%, 기존 거래소의 수수료의 두배를 고려해서 (실제 거래에서는 bid-ask spread, slippage등 요소가 많기 때문) 수익률을 계산해봤습니다.
결과적으로 다른 방법들보다 수익률이 압도적으로 좋은 모습을 보였습니다.</p><table><thead><tr><th align="center">Backtest 결과</th></tr></thead><tbody><tr><td align="center"><img class="figCenter_mKJ5" src="/assets/images/backtest_result-e8ad8121f0db0a6cf475d6b67c73df5f.png" alt="backtest"></td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_mojV" id="live-run-성능-그리고-한계점들">Live Run 성능, 그리고 한계점들<a class="hash-link" href="#live-run-성능-그리고-한계점들" title="Direct link to heading">​</a></h3><p>백테스트 성능만으로 실제 투자 수익률에 대해서 판단하기에 한계가 있다고 생각해서 논문에서 제시한 프레임워크와 모델 체크포인트를 활용해서 실제로 투자를 해보면서 수익률을 관찰해봤습니다.
약 $100 정도의 초기 시드를 가지고 Bybit라는 선물 거래소에서 BTC/USDT 투자를 시작했고 거의 2주정도의 기간동안 돌려놓은 결과 단순 buy and hold (장기투자)보다 우수한 수익률을 기록했습니다.
더 오래동안 돌려서 관찰해보고 싶었지만 논문 제출일정때문에 추가적인 관찰결과는 논문에 실을 수 없었습니다.</p><table><thead><tr><th align="center">Liverun 결과</th></tr></thead><tbody><tr><td align="center"><img class="figCenter_mKJ5" src="/assets/images/liverun_result-75baec03bfc17c29fe6b7c94ad334690.png" alt="liverun"></td></tr></tbody></table><p>몇가지 limitation들도 보였는데 일단은 take profit이 0.75%이기 때문에 게속 상승만 하는 구간에서는 단순 long이나 buy and hold 전략보다 수익률이 작을 수 있습니다. 반대로 계속 하락하는 구간에서는 단순 short 전략보다 수익률이 적을 수 있습니다.
그리고 마땅한 stop loss mechanism도 없다는게 큰 단점입니다. <a href="https://en.wikipedia.org/wiki/Secretary_problem" target="_blank" rel="noopener noreferrer">Secretary problem</a> 같은 방식을 응용한 exit 기법이나 언제 exit하는게 최적인지 판단하는 강화학습 기반 stop loss전략을 도입해봐도 좋을 것 같습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="future-work">Future Work<a class="hash-link" href="#future-work" title="Direct link to heading">​</a></h3><ul><li>단순하게 sentiment score를 활용하는 모델보다는 text들의 임베딩을 뽑아서 차트 피처의 임베딩과 모델 내부에서 결합되고, text와 chart의 임베딩을 뽑는 인코더들을 전부 end-to-end로 학습시키는 모델을 생각해볼 수 있습니다.</li><li>Sentiment classification 이외에도 crypto text semantic similarity search, crypto text NER, crypto pump and dump classification등 다양한 테스크에도 저희의 LM이 적용가능합니다. 특히 유사한 암호화폐 뉴스 서치 시스템을 현재 개발중이고 관련해서 후속 논문 작업도 진행해보고 있습니다.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h3><a name="r1"></a><ol><li>J. Abraham, D. Higdon, J. Nelson, and J. Ibarra. Cryp-tocurrency price prediction using tweet volumes and sentiment analysis. SMU Data Science Review, 1(3):1, 2018.</li></ol><a name="r2"></a><ol start="2"><li>D. Araci. Finbert: Financial sentiment analysis with pre-trained language models. arXiv preprint arXiv:1908.10063, 2019</li></ol><a name="r3"></a><ol start="3"><li>S.. Arik and T. Pfister. Tabnet: Attentive interpretable tabular learning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(8):6679–6687, May 2021. URL <a href="https://ojs.aaai.org/index.php/AAAI/article/view/16826" target="_blank" rel="noopener noreferrer">https://ojs.aaai.org/index.php/AAAI/article/view/16826</a></li></ol><a name="r4"></a><ol start="4"><li>. Barnwal, H. P. Bharti, A. Ali, and V. Singh. Stacking with neural network for cryptocurrency investment. In 2019 New York scientific data summit (NYSDS), pages 1–5. IEEE, 2019</li></ol><a name="r5"></a><ol start="5"><li>Z. Boukhers, A. Bouabdallah, M. Lohr, and J. Jürjens. Ensemble and multimodal approach for forecasting cryptocurrency price, 2022. URL <a href="https://arxiv.org/abs/2202.08967" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2202.08967</a></li></ol><a name="r6"></a><ol start="6"><li>I.Chalkiadakis, A. Zaremba, G. W. Peters, and M. J.Chantler. On-chain analytics for sentiment-driven sta-tistical causality in cryptocurrencies. Blockchain: Research and Applications, 3(2):100063, 2022. ISSN 2096-7209. . URL <a href="https://www.sciencedirect.com/science/article/pii/S2096720922000033" target="_blank" rel="noopener noreferrer">https://www.sciencedirect.com/science/article/pii/S2096720922000033</a>.</li></ol><a name="r7"></a><ol start="7"><li>Q. Chen. Stock movement prediction with financial news using contextualized embedding from bert, 2021.URL <a href="https://arxiv.org/abs/2107.08721" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2107.08721</a>.</li></ol></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/publication">publication</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/paper-review">paper-review</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/nu-wave2">NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-10-25T00:00:00.000Z" itemprop="datePublished">October 25, 2022</time> · <!-- -->16 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/Seungwoo0326" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/Seungwoo0326.png" alt="Seungu Han"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/Seungwoo0326" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Seungu Han</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio)</small></div></div></div><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/junjun3518.png" alt="Junhyeok Lee"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Junhyeok Lee</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio, Head)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/abs/2206.08545" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2206.08545-brightgreen.svg?style=flat-square" alt="arXiv"></a> <a href="https://github.com/mindslab-ai/nuwave2" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/mindslab-ai/nuwave2?color=yellow&amp;label=NU-Wave2&amp;logo=github&amp;style=flat-square" alt="GitHub Repo stars"></a> <a href="https://mindslab-ai.github.io/nuwave2/" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-audio_samples-blue?logo=Github&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Seungu Han and Junhyeok Lee. &quot;NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates&quot; </p><p>INTERSPEECH 2022</p></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="nu-wave-2-a-general-neural-audio-upsampling-model-for-various-sampling-rates">NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates<a class="hash-link" href="#nu-wave-2-a-general-neural-audio-upsampling-model-for-various-sampling-rates" title="Direct link to heading">​</a></h2><p>안녕하세요. MINDs Lab Brain에서 Audio 연구를 하고 있는 한승우입니다. 작년에 소개해드렸던 <a href="https://mindslab-ai.github.io/blog/nu-wave" target="_blank" rel="noopener noreferrer">&quot;NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling&quot;</a><a href="#r1"><sup>[1]</sup></a>의 후속 연구인 NU-Wave 2를 소개하려합니다. 앞서 <a href="https://github.com/chohyunjae1" target="_blank" rel="noopener noreferrer">현재</a>님이 소개해주신 <a href="https://mindslab-ai.github.io/blog/sane-tts" target="_blank" rel="noopener noreferrer">&quot;SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech&quot;</a>와 함께 Interspeech 2022에 accept 되어 9월 18일부터 22일까지 인천 송도에서 열린 Interspeech 2022에서 구두 발표를 마치고 왔습니다. 코로나 19로 인해 처음으로 대면으로 참석한 국제학회이면서도 많은 사람들 앞에서 발표까지 하게 되어 긴장되면서도 설레었는데 다행히 무사히 발표를 마치고 왔습니다. 또한 다른 연구자분들의 발표도 듣고 질문도 하며 많은 국내외 연구자분들과 소통할 수 있는 소중한 경험이었습니다. 그럼 이제 연구를 같이 진행해주신 <a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer">준혁</a>님과 연구 및 발표에 도움을 주신 모든 MINDs Lab Brain 팀원분들께 감사 인사 전하며 NU-Wave 2 소개를 시작하겠습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="motivations">Motivations<a class="hash-link" href="#motivations" title="Direct link to heading">​</a></h3><p>NU-Wave 2의 베이스가 되는 NU-Wave는 Diffusion Model을 적용해 최초로 오디오를 48 kHz로 upsampling 하는데 성공한 모델입니다. 더 자세한 내용은 <a href="https://mindslab-ai.github.io/blog/nu-wave" target="_blank" rel="noopener noreferrer">NU-Wave(Interspeech):</a>를 참고해주세요. 하지만 NU-Wave는 몇 가지 문제를 지니고 있는데 첫번째 문제는 &#x27;ㅅ, ㅆ, ㅊ&#x27;와 같은 치찰음이나 마찰음은 고역대에서 잘 생성해내는 반면에 harmonic 성분은 잘 만들어내지 못한다는 점입니다. 또한 몇몇 논문들에서 NU-Wave가 다른 다양한 음역대를 생성하는데에는 성능이 좋지 않다는 결과를 보여줬습니다. 그리고 두번째 문제는 NU-Wave만의 문제는 아니지만 이전 오디오 Upsampling 모델들은 input과 output의 sampling rate가 고정되어있다는 점입니다. 이렇게 input, output sampling rate 쌍을 고정해버리면 새로운 쌍에 대해서 upsampling을 하고 싶을 때는 매번 모델을 새로 학습시켜야한다는 문제가 있습니다. 이를 해결하기 위해서 저희는 어떠한 sampling rate의 input이 들어와도 원하는 하나의 sampling rate의 오디오로 upsampling해주는 문제를 의논하고 이를 &#x27;General Neural Audio Upsampling&#x27;이라고 불렀습니다. 이 문제를 해결하게 되면 만약 원하는 결과의 sampling rate가 48 kHz라면 모델을 한 번만 학습시켜도 8 kHz든 12 kHz든 혹은 16 kHz든 어떤 sampling rate의 오디오도 48 kHz로 upsampling할 수 있게 되는겁니다.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/publication">publication</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/paper-review">paper-review</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates" href="/blog/nu-wave2"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/sane-tts">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-09-02T00:00:00.000Z" itemprop="datePublished">September 2, 2022</time> · <!-- -->23 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/chohyunjae1" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/chohyunjae1.png" alt="Hyunjae Cho"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/chohyunjae1" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Hyunjae Cho</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio)</small></div></div></div><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/Wonbin-Jung" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/Wonbin-Jung.png" alt="Wonbin Jung"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/Wonbin-Jung" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Wonbin Jung</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio)</small></div></div></div><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/junjun3518.png" alt="Junhyeok Lee"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Junhyeok Lee</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio, Head)</small></div></div></div><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/tonyswoo" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/tonyswoo.png" alt="Sang Hoon Woo"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/tonyswoo" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sang Hoon Woo</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/abs/2206.12132" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2206.12132-brightgreen.svg?style=flat-square" alt="arXiv"></a>
<a href="https://mindslab-ai.github.io/sane-tts/" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Audio%20Samples&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Hyunjae Cho, Wonbin Jung, Junhyeok Lee, and Sang Hoon Woo. &quot;SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech.&quot; </p><p>INTERSPEECH 2022</p></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="sane-tts-stable-and-natural-end-to-end-multilingual-text-to-speech">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech<a class="hash-link" href="#sane-tts-stable-and-natural-end-to-end-multilingual-text-to-speech" title="Direct link to heading">​</a></h2><p>안녕하세요. MINDsLab Brain팀에서 음성합성 연구를 하고 있는 조현재입니다.</p><p>오늘은 저희 Brain팀에서 같이 쓴 SANE-TTS paper가 <a href="https://www.interspeech2022.org" target="_blank" rel="noopener noreferrer">INTERSPEECH 2022</a>에 accept되었다는 좋은 소식과 함께 소개해드리는 시간을 가지려 합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="contributions">Contributions<a class="hash-link" href="#contributions" title="Direct link to heading">​</a></h3><ul><li>SANE-TTS는 안정적이고 자연스러운 다국어 음성을 합성합니다.</li><li>이 논문에서 제안한 <em>speaker regularization loss</em>는 기존 다국어 TTS 모델에서도 사용된 방법인 domain adversarial training과 비슷한 수준의 퀄리티 향상을 이끌어냅니다.</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/publication">publication</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/paper-review">paper-review</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech" href="/blog/sane-tts"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/assem-vc">Assem-VC and Assem-Singer:</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-08-19T00:00:00.000Z" itemprop="datePublished">August 19, 2022</time> · <!-- -->14 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/wookladin" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/wookladin.png" alt="Kang-wook Kim"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/wookladin" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Kang-wook Kim</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio)</small></div></div></div><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/junjun3518.png" alt="Junhyeok Lee"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Junhyeok Lee</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio, Head)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div class="markdown" itemprop="articleBody"><p>Assem-VC</p><p><a href="https://arxiv.org/abs/2104.00931" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2104.00931-brightgreen.svg?style=flat-square" alt="arXiv"></a>
<a href="https://mindslab-ai.github.io/assem-vc/index.html" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Audio%20Samples&amp;logo=Github&amp;labelColor=grey&amp;color=lightgrey&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a>
<a href="https://github.com/mindslab-ai/assem-vc" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Official%20Repo&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><p>Assem-Singer</p><p><a href="https://arxiv.org/abs/2110.12676" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2110.12676-brightgreen.svg?style=flat-square" alt="arXiv"></a>
<a href="https://mindslab-ai.github.io/assem-vc/singer/index.html" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Audio%20Samples&amp;logo=Github&amp;labelColor=grey&amp;color=lightgrey&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><p>안녕하세요! MINDsLab Brain의 음성합성 연구를 담당한 <a href="https://kwkim.me" target="_blank" rel="noopener noreferrer">김강욱</a>입니다.</p><p>오늘은 저희 Brain 팀에서 출판한 논문인 <strong>Assem-VC<a href="#r1">[1]</a>와 Assem-Singer<a href="#r2">[2]</a></strong>를 소개드리는 시간을 가지려 합니다!</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/publication">publication</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/paper-review">paper-review</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Assem-VC and Assem-Singer:" href="/blog/assem-vc"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/ces-2022-review">CES 2022에 참석한 마인즈랩, AI Human</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-02-17T00:00:00.000Z" itemprop="datePublished">February 17, 2022</time> · <!-- -->18 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/deepkyu.png" alt="Hyoung-Kyu Song"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Hyoung-Kyu Song</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Vision, Head)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_mojV" id="brain-team-at-ces-2022">Brain Team at CES 2022<a class="hash-link" href="#brain-team-at-ces-2022" title="Direct link to heading">​</a></h2><p>안녕하세요! 마인즈랩 Brain팀 Vision 파트 연구원 송형규입니다.<br>
<!-- -->지난 1월 5일부터 7일까지 3일동안 미국 라스베가스에서 세계 최고 가전 박람회인 CES 2022가 열렸습니다. 2년만에 오프라인으로 다시 돌아온 이번 CES 2022에는 오미크론이 확산되면서 평년 대비 많은 인원이 참석하지는 않았지만, 워낙 큰 박람회여서 그런지 그 규모는 대단했습니다.<br>
<!-- -->마인즈랩은 이번 CES 2022에서 국내 은행과 함께 구현하여 현재 상용에 도입된 AI Banker를 전시했습니다. 저는 이번에 기술 설명을 위해 CES 출장길에 동행했는데요. 박람회인만큼 연구자들이 중심이 되는 딥러닝/머신러닝 학회와는 완전히 다른 분위기지만, 투자자들과 바이어, 일반 참석자들이 참석하는 자리이다 보니까 매의 눈으로 기능 하나하나에 대해 여쭤보시는 분들이 더욱 많아 개인적으로 논문 발표하는 것보다 더 어려웠습니다(다녀오니까 살이 2kg 빠졌어요 ㅎㅎ;).<br>
<!-- -->이번 포스트에서는 CES 2022에서의 분위기를 간접적으로나마 전달드리고, 이번에 AI Human에 도입된 기술들에 대해 설명드리겠습니다. 이번 글은 기술 블로그이기는 하나, 기술적인 내용을 다루기보다는 딥러닝으로 해결한 Task가 어떤 것이었는지를 말씀드리려고 합니다 😄</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/news">news</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about CES 2022에 참석한 마인즈랩, AI Human" href="/blog/ces-2022-review"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/vits">VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-19T00:00:00.000Z" itemprop="datePublished">October 19, 2021</time> · <!-- -->32 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/Wonbin-Jung" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/Wonbin-Jung.png" alt="Wonbin Jung"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/Wonbin-Jung" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Wonbin Jung</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/abs/2106.06103" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2106.06103-brightgreen.svg?style=flat-square" alt="arXiv"></a>
<a href="https://github.com/jaywalnut310/vits" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Official%20Repo&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a>
<a href="https://jaywalnut310.github.io/vits-demo/index.html" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Audio%20Samples&amp;logo=Github&amp;labelColor=grey&amp;color=lightgrey&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Kim, Jaehyeon, Jungil Kong, and Juhee Son. &quot;Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.&quot; </p><p>International Conference on Machine Learning (ICML) 2021</p></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="variational-inference-with-adversarial-learning-for-end-to-end-text-to-speech">Variational Inference with adversarial learning for end-to-end Text-to-Speech<a class="hash-link" href="#variational-inference-with-adversarial-learning-for-end-to-end-text-to-speech" title="Direct link to heading">​</a></h2><p>안녕하세요! MINDs Lab Brain에서 text-to-speech (TTS) 연구를 하고 있는 정원빈입니다. 오늘은 지난 여름에 발표된 end-to-end TTS인 Variational Inference with adversarial learning for end-to-end Text-to-Speech, <strong>VITS</strong>에 대해 소개하고, 리뷰를 진행하고자 합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="contributions">Contributions<a class="hash-link" href="#contributions" title="Direct link to heading">​</a></h3><ul><li>1단계 합성 및 병렬 트레이닝이 가능하며 성능이 기존 모델에 견줄 수 있는 end-to-end TTS를 제안했습니다.</li><li>Variational Auto-Encoder (VAE)의 구조를 적용하여 2단계 합성을 하나로 연결시켰습니다.</li><li>Variational inference에 normalizing flow와 generative adversarial network (GAN)의 adversarial training을 결합시켜 표현력을 높였습니다.</li><li>Stochastic duration predictor (SDP)를 사용하여 랜덤하게 음성의 길이를 예측하므로 음성의 다양성이 향상되었습니다.</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/paper-review">paper-review</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech" href="/blog/vits"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/acon">Activate or Not: Learning Customized Activation</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-07-19T00:00:00.000Z" itemprop="datePublished">July 19, 2021</time> · <!-- -->15 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/deepkyu.png" alt="Hyoung-Kyu Song"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Hyoung-Kyu Song</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Vision, Head)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/abs/2009.04759" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2009.04759-brightgreen.svg?style=flat-square" alt="arXiv"></a>
<a href="https://github.com/nmaac/acon" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Official%20Repo&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Ma, Ningning, et al. &quot;Activate or Not: Learning Customized Activation.&quot;<br>
<!-- -->Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="먼저-알면-좋은-것들">먼저 알면 좋은 것들<a class="hash-link" href="#먼저-알면-좋은-것들" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="swish-activation-function-sup1sup">Swish Activation Function <a href="#r1"><sup>[1]</sup></a><a class="hash-link" href="#swish-activation-function-sup1sup" title="Direct link to heading">​</a></h3><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">swish</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mi>x</mi><mo>×</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>β</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>x</mi><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>β</mi><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\operatorname{swish}(x):=x \times \sigma(\beta x)=\frac{x}{1+e^{-\beta x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mord mathrm">swish</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.8769em;vertical-align:-0.7693em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7751em"><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><img class="figCenter_mKJ5" src="/assets/images/figure1_swish-c594f427f7c7dacfa81c1f6d97e10eb5.png" alt="figure1_swish"><ul><li>Linear Function과 ReLU 사이에서의 non-linearly interpolated activation을 보여줍니다.<ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">β = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span></span> 일 경우, Linear function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">f(x) = x/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">x</span><span class="mord">/2</span></span></span></span></span> 처럼 작용하게 됩니다.</li><li>반대로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">β → ∞</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord">∞</span></span></span></span></span>일 경우, Sigmoid에 해당하는 부분이 0-1 activation처럼 작용하게 되어, Swish가 ReLU처럼 작용하게 됩니다.</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">β = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>일 경우, 강화학습에서 사용되는 Sigmoid-weighted Linear Unit (SiL) function처럼 작용할 수 있습니다.</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">β</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>는 위에서 보신 것처럼 어떤 상수일 수도 있고, 모델에 따라서는 훈련 가능한 파라미터가 될 수도 있습니다.</li></ul></li><li>브레인팀 AI Scientist분들이 자주 사용하시는 Activation Function이기도 하죠 🙂</li><li>Generative Model에서도 ReLU 대신 사용하는 경우가 많이 있습니다.</li><li>최근에는 Implicit Representation Network 상에서도 Swish가 다시금 주목을 받고 있습니다.<ul><li>SIREN에서 언급하는 periodic function activation (Sine 함수 등) 보다 Swish가 더 나은 성능을 보이는 Task가 있습니다.</li></ul></li></ul></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/paper-review">paper-review</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Activate or Not: Learning Customized Activation" href="/blog/acon"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/nu-wave">NU-Wave(Interspeech):</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-07-14T00:00:00.000Z" itemprop="datePublished">July 14, 2021</time> · <!-- -->16 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/junjun3518.png" alt="Junhyeok Lee"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Junhyeok Lee</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio, Head)</small></div></div></div><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/Seungwoo0326" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/Seungwoo0326.png" alt="Seungu Han"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/Seungwoo0326" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Seungu Han</span></a></div><small class="avatar__subtitle" itemprop="description">AI Scientist (Audio)</small></div></div></div></div></header><meta itemprop="image" content="https://maum-ai.github.io//img/mindslab_default.png"><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/abs/2104.02321" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2104.02321-brightgreen.svg?style=flat-square" alt="arXiv"></a> <a href="https://github.com/mindslab-ai/nuwave" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/mindslab-ai/nuwave?color=yellow&amp;label=NU-Wave&amp;logo=github&amp;style=flat-square" alt="GitHub Repo stars"></a> <a href="https://mindslab-ai.github.io/nuwave/" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-audio_samples-blue?logo=Github&amp;style=flat-square" alt="githubio"></a></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="a-diffusion-probabilistic-model-for-neural-audio-upsampling">A Diffusion Probabilistic Model for Neural Audio Upsampling<a class="hash-link" href="#a-diffusion-probabilistic-model-for-neural-audio-upsampling" title="Direct link to heading">​</a></h2><p>안녕하세요 MINDs Lab Brain에서 Audio와 Speech 연구를 하고 있는 <a href="https://github.com/junjun3518" target="_blank" rel="noopener noreferrer">이준혁</a>입니다. Audio domain의 딥러닝 연구는 대부분 <em>Sampling Rate</em> 16kHz인 경우에 대해서 진행되었습니다. 생성모델 (TTS) 같은 경우 22.05kHz인 경우도 있었지만 음악이나 영화 쪽에서 많이 쓰이는 44.1kHz나 48kHz에 비해서는 절반밖에 안 되는 sampling rate이었기 때문에 high sampling rate TTS에 대한 수요가 꾸준히 있었습니다. 저희가 개발한 TTS에 대해서 더 높은 퀄리티로 서비스를 제공하기 위해서 <em>Neural Audio Upsampling</em>에 대한 연구를 진행하게 되었습니다. 48kHz라는 높은 sampling rate를 목표로 하는 연구를 진행하다 보니 자연스럽게 최근에 핫한 생성모델인 <em>diffusion model</em>을 적용하게 되었고 <strong>최초로 48kHz를 target으로 upsampling 하는데 성공</strong>하였습니다. 이렇게 진행한 연구로 <a href="https://github.com/Seungwoo0326" target="_blank" rel="noopener noreferrer">승우</a>님과 같이 쓴 paper가 <strong>세계 최고의 음성신호처리학회인 <a href="https://www.interspeech2021.org" target="_blank" rel="noopener noreferrer">INTERSPEECH 2021</a>에 Accept</strong>🎉 되어 소개해 드리고자 합니다!</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/publication">publication</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/paper-review">paper-review</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about NU-Wave(Interspeech):" href="/blog/nu-wave"><b>Read More</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Contents</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/publications">Publications</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/maum-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/careers">Careers</a></li><li class="footer__item"><a class="footer__link-item" href="/blog/tags">Tags</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 maum.ai BRAIN Team. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.8a7caefc.js"></script>
<script src="/assets/js/main.e6055739.js"></script>
</body>
</html>