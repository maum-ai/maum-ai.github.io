<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="MINDsLab BRAIN Team RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="MINDsLab BRAIN Team Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-204903244-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">Publications | MINDsLab BRAIN Team</title><meta data-rh="true" property="og:title" content="Publications | MINDsLab BRAIN Team"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" name="description" content="List of Publications from MINDsLab Brain Team"><meta data-rh="true" property="og:description" content="List of Publications from MINDsLab Brain Team"><meta data-rh="true" property="og:url" content="https://mindslab-ai.github.io//publications"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://mindslab-ai.github.io//publications"><link data-rh="true" rel="alternate" href="https://mindslab-ai.github.io//publications" hreflang="en"><link data-rh="true" rel="alternate" href="https://mindslab-ai.github.io//publications" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.9bd3b1b6.css">
<link rel="preload" href="/assets/js/runtime~main.12fa59cb.js" as="script">
<link rel="preload" href="/assets/js/main.b14b4538.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/mindslab_brain.svg" alt="MINDsLab BRAIN Team" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/mindslab_brain.svg" alt="MINDsLab BRAIN Team" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title"></b></a><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/open-source">Open-Source</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/careers">Careers</a><a href="https://github.com/mindslab-ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper mdx-wrapper mdx-page"><main class="container container--fluid margin-vert--lg"><div class="row mdxPageWrapper_zHyg"><div class="col col--8"><h2 class="anchor anchorWithStickyNavbar_mojV" id="publications">Publications<a class="hash-link" href="#publications" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="2022">2022<a class="hash-link" href="#2022" title="Direct link to heading">â€‹</a></h3><section id="activities" class="category_GWcC"><ul class="publications_acVk"><li><div class="conference_QDfE">Interspeech</div><h3><a href="https://arxiv.org/abs/2206.12132" target="_blank" rel="noopener noreferrer">SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech</a></h3><p class="authors_KmdO"><b>Hyunjae Cho<sup>*</sup></b>, <b>Wonbin Jung</b>, <b>Junhyeok Lee</b>, <!-- -->and <b>Sang Hoon Woo</b></p><div><input type="checkbox" class="expanded_RYzr" id="1f32d4c2-ad2d-4193-8d86-c9a2568b4718"><p class="description_MqPA">In this paper, we present SANE-TTS, a stable and natural end-to-end multilingual TTS model. <span class="full_aKhn">By the difficulty of obtaining multilingual corpus for given speaker, training multilingual TTS model with monolingual corpora is unavoidable. We introduce speaker regularization loss that improves speech naturalness during cross-lingual synthesis as well as domain adversarial training, which is applied in other multilingual TTS models. Furthermore, by adding speaker regularization loss, replacing speaker embedding with zero vector in duration predictor stabilizes cross-lingual inference. With this replacement, our model generates speeches with moderate rhythm regardless of source speaker in cross-lingual synthesis. In MOS evaluation, SANE-TTS achieves naturalness score above 3.80 both in cross-lingual and intralingual synthesis, where the ground truth score is 3.99. Also, SANE-TTS maintains speaker similarity close to that of ground truth even in cross-lingual inference. Audio samples are available on our web page.</span></p><label for="1f32d4c2-ad2d-4193-8d86-c9a2568b4718" class="trigger_WWqs"></label></div><p class="demo_M62R"><a href="https://mindslab-ai.github.io/sane-tts/" target="_blank" rel="noopener noreferrer">Demo</a></p></li><li><div class="conference_QDfE">Interspeech</div><h3><a href="https://arxiv.org/abs/2206.08545" target="_blank" rel="noopener noreferrer">NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates</a></h3><p class="authors_KmdO"><b>Seungu Han<sup>*</sup></b> <!-- -->and <b>Junhyeok Lee</b></p><div><input type="checkbox" class="expanded_RYzr" id="84ffab9f-72e9-4657-955c-651409680d42"><p class="description_MqPA">Conventionally, audio super-resolution models fixed the initial and the target sampling rates, which necessitate the model to be trained for each pair of sampling rates. <span class="full_aKhn">We introduce NU-Wave 2, a diffusion model for neural audio upsampling that enables the generation of 48 kHz audio signals from inputs of various sampling rates with a single model. Based on the architecture of NU-Wave, NU-Wave 2 uses short-time Fourier convolution (STFC) to generate harmonics to resolve the main failure modes of NU-Wave, and incorporates bandwidth spectral feature transform (BSFT) to condition the bandwidths of inputs in the frequency domain. We experimentally demonstrate that NU-Wave 2 produces high-resolution audio regardless of the sampling rate of input while requiring fewer parameters than other models.</span></p><label for="84ffab9f-72e9-4657-955c-651409680d42" class="trigger_WWqs"></label></div><p class="github_vEIJ"><a href="https://github.com/mindslab-ai/nuwave2" target="_blank" rel="noopener noreferrer">Github</a></p><p class="demo_M62R"><a href="https://mindslab-ai.github.io/nuwave2/" target="_blank" rel="noopener noreferrer">Demo</a></p></li><li><div class="conference_QDfE">PR Letters</div><h3><a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865522001076" target="_blank" rel="noopener noreferrer">Zero-Shot Semantic Segmentation via Spatial and Multi-Scale Aware Visual Class Embedding</a></h3><p class="authors_KmdO"><b>Sungguk Cha<sup>*</sup></b> <!-- -->and <!-- -->Yooseung Wang<sup>*</sup></p><div><input type="checkbox" class="expanded_RYzr" id="99d3c632-932d-4c4a-ac37-e1d1728c59ec"><p class="description_MqPA">Fully supervised semantic segmentation technologies bring a paradigm shift in scene understanding. <span class="full_aKhn">However, the burden of expensive labeling cost remains as a challenge. To solve the cost problem, recent studies proposed language model based zero-shot semantic segmentation (L-ZSSS) approaches. In this paper, we address L-ZSSS has a limitation in generalization which is a virtue of zero-shot learning. Tackling the limitation, we propose a language-model-free zero-shot semantic segmentation framework, Spatial and Multi-scale aware Visual Class Embedding Network (SM-VCENet). Furthermore, leveraging vision-oriented class embedding SM-VCENet enriches visual information of the class embedding by multi-scale attention and spatial attention. We also propose a novel benchmark (PASCAL2COCO) for zero-shot semantic segmentation, which provides generalization evaluation by domain adaptation and contains visually challenging samples. In experiments, our SM-VCENet outperforms zero-shot semantic segmentation state-of-the-art by a significant margin in both PASCAL-5i and PASCAL2COCO benchmarks.</span></p><label for="99d3c632-932d-4c4a-ac37-e1d1728c59ec" class="trigger_WWqs"></label></div></li><li><div class="conference_QDfE">CVPR Demo</div><h3><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Song_Talking_Face_Generation_With_Multilingual_TTS_CVPR_2022_paper.html" target="_blank" rel="noopener noreferrer">Talking Face Generation with Multilingual TTS</a></h3><p class="authors_KmdO"><b>Hyoung-Kyu Song<sup>*</sup></b>, <b>Sang Hoon Woo<sup>*</sup></b>, <b>Junhyeok Lee</b>, <b>Seungmin Yang</b>, <b>Hyunjae Cho</b>, <b>Dongho Choi</b>, <b>Kang-wook Kim</b>, <!-- -->and <b>Youseong Lee</b></p><div><input type="checkbox" class="expanded_RYzr" id="bc2599d1-6e27-48c4-9b83-5733b3a0bcb7"><p class="description_MqPA">In this work, we propose a joint system combining a talking face generation system with a text-to-speech system that can generate multilingual talking face videos from only the text input. <span class="full_aKhn">Our system can synthesize natural multilingual speeches while maintaining the vocal identity of the speaker, as well as lip movements synchronized to the synthesized speech. We demonstrate the generalization capabilities of our system by selecting four languages (Korean, English, Japanese, and Chinese) each from a different language family. We also compare the outputs of our talking face generation model to outputs of a prior work that claims multilingual support. For our demo, we add a translation API to the preprocessing stage and present it in the form of a neural dubber so that users can utilize the multilingual property of our system more easily.</span></p><label for="bc2599d1-6e27-48c4-9b83-5733b3a0bcb7" class="trigger_WWqs"></label></div><p class="demo_M62R"><a href="https://huggingface.co/spaces/CVPR/ml-talking-face" target="_blank" rel="noopener noreferrer">ðŸ¤—Demo</a></p><p class="misc_vWNW"><a href="https://www.youtube.com/watch?v=toqdD1F_ZsU" target="_blank" rel="noopener noreferrer">Screencast</a></p></li><li><div class="conference_QDfE">CVPR</div><h3><a href="https://arxiv.org/abs/2106.07023" target="_blank" rel="noopener noreferrer">Styleformer: Transformer based Generative Adversarial Networks with Style Vector</a></h3><p class="authors_KmdO">Jeeseung Park<sup>*</sup> <!-- -->and <b>Younggeun Kim<sup>*</sup></b></p><div><input type="checkbox" class="expanded_RYzr" id="bd353245-aff0-4ead-9bd7-74737e8a3c2f"><p class="description_MqPA">We propose Styleformer, which is a style-based generator for GAN architecture, but a convolution-free transformer-based generator. <span class="full_aKhn">In our paper, we explain how a transformer can generate high-quality images, overcoming the disadvantage that convolution operations are difficult to capture global features in an image. Furthermore, we change the demodulation of StyleGAN2 and modify the existing transformer structure (e.g., residual connection, layer normalization) to create a strong style-based generator with a convolution-free structure. We also make Styleformer lighter by applying Linformer, enabling Styleformer to generate higher resolution images and result in improvements in terms of speed and memory. We experiment with the low-resolution image dataset such as CIFAR-10, as well as the high-resolution image dataset like LSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark dataset, which is comparable performance to the current state-of-the-art and outperforms all GAN-based generative models, including StyleGAN2-ADA with fewer parameters on the unconditional setting. We also both achieve new state-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10 and CelebA.</span></p><label for="bd353245-aff0-4ead-9bd7-74737e8a3c2f" class="trigger_WWqs"></label></div><p class="github_vEIJ"><a href="https://github.com/Jeeseung-Park/Styleformer" target="_blank" rel="noopener noreferrer">Github</a></p></li><li><div class="conference_QDfE">ICASSP</div><h3><a href="https://arxiv.org/abs/2104.00931" target="_blank" rel="noopener noreferrer">Assem-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques</a></h3><p class="authors_KmdO"><b>Kang-wook Kim<sup>*</sup></b>, <b>Seung-won Park</b>, <b>Junhyeok Lee</b>, <!-- -->and <b>Myun-chul Joe</b></p><div><input type="checkbox" class="expanded_RYzr" id="eda4de0e-a049-40ef-a9d8-2ced9c0369fb"><p class="description_MqPA">Recent works on voice conversion (VC) focus on preserving the rhythm and the intonation as well as the linguistic content. <span class="full_aKhn">To preserve these features from the source, we decompose current non-parallel VC systems into two encoders and one decoder. We analyze each module with several experiments and reassemble the best components to propose Assem-VC, a new state-of-the-art any-to-many non-parallel VC system. We also examine that PPG and Cotatron features are speaker-dependent, and attempt to remove speaker identity with adversarial training.</span></p><label for="eda4de0e-a049-40ef-a9d8-2ced9c0369fb" class="trigger_WWqs"></label></div><p class="github_vEIJ"><a href="https://github.com/mindslab-ai/assem-vc" target="_blank" rel="noopener noreferrer">Github</a></p></li></ul></section><h3 class="anchor anchorWithStickyNavbar_mojV" id="2021">2021<a class="hash-link" href="#2021" title="Direct link to heading">â€‹</a></h3><section id="activities" class="category_GWcC"><ul class="publications_acVk"><li><div class="conference_QDfE">NeurIPS Workshop (Oral)</div><h3><a href="https://arxiv.org/abs/2110.12676" target="_blank" rel="noopener noreferrer">Controllable and Interpretable Singing Voice Decomposition via Assem-VC</a></h3><p class="authors_KmdO"><b>Kang-wook Kim<sup>*</sup></b> <!-- -->and <b>Junhyeok Lee</b></p><div><input type="checkbox" class="expanded_RYzr" id="26168654-ae6f-4d30-bc08-03b08f693966"><p class="description_MqPA">We propose a singing decomposition system that encodes time-aligned linguistic content, pitch, and source speaker identity via Assem-VC. <span class="full_aKhn">With decomposed speaker-independent information and the target speaker&#x27;s embedding, we could synthesize the singing voice of the target speaker. In conclusion, we made a perfectly synced duet with the user&#x27;s singing voice and the target singer&#x27;s converted singing voice.</span></p><label for="26168654-ae6f-4d30-bc08-03b08f693966" class="trigger_WWqs"></label></div><p class="github_vEIJ"><a href="https://github.com/mindslab-ai/assem-vc" target="_blank" rel="noopener noreferrer">Github</a></p><p class="demo_M62R"><a href="https://mindslab-ai.github.io/assem-vc/singer/" target="_blank" rel="noopener noreferrer">Demo</a></p></li><li><div class="conference_QDfE">Interspeech</div><h3><a href="https://arxiv.org/abs/2104.02321" target="_blank" rel="noopener noreferrer">NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling</a></h3><p class="authors_KmdO"><b>Junhyeok Lee<sup>*</sup></b> <!-- -->and <b>Seungu Han</b></p><div><input type="checkbox" class="expanded_RYzr" id="c4e28b75-da28-4f62-b6d0-860f8de70595"><p class="description_MqPA">In this work, we introduce NU-Wave, the first neural audio upsampling model to produce waveforms of sampling rate 48kHz from coarse 16kHz or 24kHz inputs, while prior works could generate only up to 16kHz. <span class="full_aKhn">NU-Wave is the first diffusion probabilistic model for audio super-resolution which is engineered based on neural vocoders. NU-Wave generates high-quality audio that achieves high performance in terms of signal-to-noise ratio (SNR), log-spectral distance (LSD), and accuracy of the ABX test. In all cases, NU-Wave outperforms the baseline models despite the substantially smaller model capacity (3.0M parameters) than baselines (5.4-21%).</span></p><label for="c4e28b75-da28-4f62-b6d0-860f8de70595" class="trigger_WWqs"></label></div><p class="github_vEIJ"><a href="https://github.com/mindslab-ai/nuwave" target="_blank" rel="noopener noreferrer">Github</a></p><p class="demo_M62R"><a href="https://mindslab-ai.github.io/nuwave" target="_blank" rel="noopener noreferrer">Demo</a></p></li><li><div class="conference_QDfE">CVPR Workshop</div><h3><a>Sharp Edge Recovery via SE(3)-Equivariant Network</a></h3><p class="authors_KmdO"><b>Youseong Lee<sup>*</sup></b></p><p class="misc_vWNW"><a href="https://youtu.be/UVYQzQ-mH1w?t=9231" target="_blank" rel="noopener noreferrer">Talk</a></p></li></ul></section><h3 class="anchor anchorWithStickyNavbar_mojV" id="2020">2020<a class="hash-link" href="#2020" title="Direct link to heading">â€‹</a></h3><section id="activities" class="category_GWcC"><ul class="publications_acVk"><li><div class="conference_QDfE">NeurIPS Workshop</div><h3><a>DS4C Patient Policy Province Dataset: A Comprehensive COVID-19 Dataset for Causal and Epidemiological Analysis</a></h3><p class="authors_KmdO">Jimi Kim<sup>*</sup>, <b>DongHwan Jang</b>, <!-- -->Seojin Jang<!-- -->, <!-- -->Woncheol Lee<!-- -->, <!-- -->and <b>Joong Kun Lee</b></p><p class="misc_vWNW"><a href="https://www.cmu.edu/dietrich/causality/neurips20ws/" target="_blank" rel="noopener noreferrer">Workshop</a></p></li><li><div class="conference_QDfE">Interspeech</div><h3><a href="https://arxiv.org/abs/2005.03295" target="_blank" rel="noopener noreferrer">Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion without Parallel Data</a></h3><p class="authors_KmdO"><b>Seung-won Park<sup>*</sup></b>, <b>Doo-young Kim</b>, <!-- -->and <b>Myun-chul Joe</b></p><div><input type="checkbox" class="expanded_RYzr" id="fa0d5742-75e9-4caf-9582-0219c45e634b"><p class="description_MqPA">We propose Cotatron, a transcription-guided speech encoder for speaker-independent linguistic representation. <span class="full_aKhn">Cotatron is based on the multispeaker TTS architecture and can be trained with conventional TTS datasets. We train a voice conversion system to reconstruct speech with Cotatron features, which is similar to the previous methods based on Phonetic Posteriorgram (PPG). By training and evaluating our system with 108 speakers from the VCTK dataset, we outperform the previous method in terms of both naturalness and speaker similarity. Our system can also convert speech from speakers that are unseen during training, and utilize ASR to automate the transcription with minimal reduction of the performance.</span></p><label for="fa0d5742-75e9-4caf-9582-0219c45e634b" class="trigger_WWqs"></label></div><p class="github_vEIJ"><a href="https://github.com/mindslab-ai/cotatron" target="_blank" rel="noopener noreferrer">Github</a></p><p class="demo_M62R"><a href="https://mindslab-ai.github.io/cotatron/" target="_blank" rel="noopener noreferrer">Demo</a></p><p class="misc_vWNW"><a href="https://youtu.be/lnNuL8hqoh4" target="_blank" rel="noopener noreferrer">Talk</a></p></li><li><div class="conference_QDfE">ECCV Workshop</div><h3><a href="https://arxiv.org/abs/2009.02857" target="_blank" rel="noopener noreferrer">3D Room Layout Estimation Beyond the Manhattan World Assumption</a></h3><p class="authors_KmdO"><b>Dongho Choi<sup>*</sup></b></p><div><input type="checkbox" class="expanded_RYzr" id="d32bab1a-f042-4b96-b517-47b7d0ac8a8c"><p class="description_MqPA">Predicting 3D room layout from single image is a challenging task with many applications. <span class="full_aKhn">In this paper, we propose a new training and post-processing method for 3D room layout estimation, built on a recent state-of-the-art 3D room layout estimation model. Experimental results show our method outperforms state-of-the-art approaches by a large margin in predicting visible room layout. Our method has obtained the 3rd place in 2020 Holistic Scene Structures for 3D Vision Workshop.</span></p><label for="d32bab1a-f042-4b96-b517-47b7d0ac8a8c" class="trigger_WWqs"></label></div></li></ul></section></div><div class="col col--2"><div class="tableOfContents_cNA8 thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#publications" class="table-of-contents__link toc-highlight">Publications</a><ul><li><a href="#2022" class="table-of-contents__link toc-highlight">2022</a></li><li><a href="#2021" class="table-of-contents__link toc-highlight">2021</a></li><li><a href="#2020" class="table-of-contents__link toc-highlight">2020</a></li></ul></li></ul></div></div></div></main></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Contents</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/publications">Publications</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/mindslab-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/careers">Careers</a></li><li class="footer__item"><a class="footer__link-item" href="/blog/tags">Tags</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 MINDsLab BRAIN Team. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.12fa59cb.js"></script>
<script src="/assets/js/main.b14b4538.js"></script>
</body>
</html>